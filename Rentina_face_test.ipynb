{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuongvmu/GCL_DemoCode/blob/main/Rentina_face_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/serengil/deepface_models/releases/download/v1.0/retinaface.h5"
      ],
      "metadata": {
        "id": "S66P-OP1q6-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow matplotlib\n",
        "!pip install tensorflow[and-cuda]==2.5.0\n",
        "!pip install retinaface"
      ],
      "metadata": {
        "id": "A5dmpgzeq7Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "from tensorflow.python.saved_model import tag_constants\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions"
      ],
      "metadata": {
        "id": "d783ACMjrs5z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_version = int(tf.__version__.split(\".\", maxsplit=1)[0])\n",
        "\n",
        "if tf_version == 1:\n",
        "    from keras.models import Model\n",
        "    from keras.layers import (\n",
        "        Input,\n",
        "        BatchNormalization,\n",
        "        ZeroPadding2D,\n",
        "        Conv2D,\n",
        "        ReLU,\n",
        "        MaxPool2D,\n",
        "        Add,\n",
        "        UpSampling2D,\n",
        "        concatenate,\n",
        "        Softmax,\n",
        "    )\n",
        "\n",
        "else:\n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.layers import (\n",
        "        Input,\n",
        "        BatchNormalization,\n",
        "        ZeroPadding2D,\n",
        "        Conv2D,\n",
        "        ReLU,\n",
        "        MaxPool2D,\n",
        "        Add,\n",
        "        UpSampling2D,\n",
        "        concatenate,\n",
        "        Softmax,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "Uu3musJ-tvi4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(exact_file) -> Model:\n",
        "    \"\"\"\n",
        "    Build RetinaFace model\n",
        "    \"\"\"\n",
        "    data = Input(dtype=tf.float32, shape=(None, None, 3), name=\"data\")\n",
        "\n",
        "    bn_data = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn_data\", trainable=False)(\n",
        "        data\n",
        "    )\n",
        "\n",
        "    conv0_pad = ZeroPadding2D(padding=tuple([3, 3]))(bn_data)\n",
        "\n",
        "    conv0 = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(7, 7),\n",
        "        name=\"conv0\",\n",
        "        strides=[2, 2],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(conv0_pad)\n",
        "\n",
        "    bn0 = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn0\", trainable=False)(conv0)\n",
        "\n",
        "    relu0 = ReLU(name=\"relu0\")(bn0)\n",
        "\n",
        "    pooling0_pad = ZeroPadding2D(padding=tuple([1, 1]))(relu0)\n",
        "\n",
        "    pooling0 = MaxPool2D((3, 3), (2, 2), padding=\"valid\", name=\"pooling0\")(pooling0_pad)\n",
        "\n",
        "    stage1_unit1_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn1\", trainable=False\n",
        "    )(pooling0)\n",
        "\n",
        "    stage1_unit1_relu1 = ReLU(name=\"stage1_unit1_relu1\")(stage1_unit1_bn1)\n",
        "\n",
        "    stage1_unit1_conv1 = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage1_unit1_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit1_relu1)\n",
        "\n",
        "    stage1_unit1_sc = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage1_unit1_sc\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit1_relu1)\n",
        "\n",
        "    stage1_unit1_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn2\", trainable=False\n",
        "    )(stage1_unit1_conv1)\n",
        "\n",
        "    stage1_unit1_relu2 = ReLU(name=\"stage1_unit1_relu2\")(stage1_unit1_bn2)\n",
        "\n",
        "    stage1_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit1_relu2)\n",
        "\n",
        "    stage1_unit1_conv2 = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage1_unit1_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit1_conv2_pad)\n",
        "\n",
        "    stage1_unit1_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn3\", trainable=False\n",
        "    )(stage1_unit1_conv2)\n",
        "\n",
        "    stage1_unit1_relu3 = ReLU(name=\"stage1_unit1_relu3\")(stage1_unit1_bn3)\n",
        "\n",
        "    stage1_unit1_conv3 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage1_unit1_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit1_relu3)\n",
        "\n",
        "    plus0_v1 = Add()([stage1_unit1_conv3, stage1_unit1_sc])\n",
        "\n",
        "    stage1_unit2_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn1\", trainable=False\n",
        "    )(plus0_v1)\n",
        "\n",
        "    stage1_unit2_relu1 = ReLU(name=\"stage1_unit2_relu1\")(stage1_unit2_bn1)\n",
        "\n",
        "    stage1_unit2_conv1 = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage1_unit2_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit2_relu1)\n",
        "\n",
        "    stage1_unit2_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn2\", trainable=False\n",
        "    )(stage1_unit2_conv1)\n",
        "\n",
        "    stage1_unit2_relu2 = ReLU(name=\"stage1_unit2_relu2\")(stage1_unit2_bn2)\n",
        "\n",
        "    stage1_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit2_relu2)\n",
        "\n",
        "    stage1_unit2_conv2 = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage1_unit2_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit2_conv2_pad)\n",
        "\n",
        "    stage1_unit2_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn3\", trainable=False\n",
        "    )(stage1_unit2_conv2)\n",
        "\n",
        "    stage1_unit2_relu3 = ReLU(name=\"stage1_unit2_relu3\")(stage1_unit2_bn3)\n",
        "\n",
        "    stage1_unit2_conv3 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage1_unit2_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit2_relu3)\n",
        "\n",
        "    plus1_v2 = Add()([stage1_unit2_conv3, plus0_v1])\n",
        "\n",
        "    stage1_unit3_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn1\", trainable=False\n",
        "    )(plus1_v2)\n",
        "\n",
        "    stage1_unit3_relu1 = ReLU(name=\"stage1_unit3_relu1\")(stage1_unit3_bn1)\n",
        "\n",
        "    stage1_unit3_conv1 = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage1_unit3_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit3_relu1)\n",
        "\n",
        "    stage1_unit3_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn2\", trainable=False\n",
        "    )(stage1_unit3_conv1)\n",
        "\n",
        "    stage1_unit3_relu2 = ReLU(name=\"stage1_unit3_relu2\")(stage1_unit3_bn2)\n",
        "\n",
        "    stage1_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit3_relu2)\n",
        "\n",
        "    stage1_unit3_conv2 = Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage1_unit3_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit3_conv2_pad)\n",
        "\n",
        "    stage1_unit3_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn3\", trainable=False\n",
        "    )(stage1_unit3_conv2)\n",
        "\n",
        "    stage1_unit3_relu3 = ReLU(name=\"stage1_unit3_relu3\")(stage1_unit3_bn3)\n",
        "\n",
        "    stage1_unit3_conv3 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage1_unit3_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage1_unit3_relu3)\n",
        "\n",
        "    plus2 = Add()([stage1_unit3_conv3, plus1_v2])\n",
        "\n",
        "    stage2_unit1_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn1\", trainable=False\n",
        "    )(plus2)\n",
        "\n",
        "    stage2_unit1_relu1 = ReLU(name=\"stage2_unit1_relu1\")(stage2_unit1_bn1)\n",
        "\n",
        "    stage2_unit1_conv1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit1_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit1_relu1)\n",
        "\n",
        "    stage2_unit1_sc = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit1_sc\",\n",
        "        strides=[2, 2],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit1_relu1)\n",
        "\n",
        "    stage2_unit1_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn2\", trainable=False\n",
        "    )(stage2_unit1_conv1)\n",
        "\n",
        "    stage2_unit1_relu2 = ReLU(name=\"stage2_unit1_relu2\")(stage2_unit1_bn2)\n",
        "\n",
        "    stage2_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit1_relu2)\n",
        "\n",
        "    stage2_unit1_conv2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage2_unit1_conv2\",\n",
        "        strides=[2, 2],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit1_conv2_pad)\n",
        "\n",
        "    stage2_unit1_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn3\", trainable=False\n",
        "    )(stage2_unit1_conv2)\n",
        "\n",
        "    stage2_unit1_relu3 = ReLU(name=\"stage2_unit1_relu3\")(stage2_unit1_bn3)\n",
        "\n",
        "    stage2_unit1_conv3 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit1_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit1_relu3)\n",
        "\n",
        "    plus3 = Add()([stage2_unit1_conv3, stage2_unit1_sc])\n",
        "\n",
        "    stage2_unit2_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn1\", trainable=False\n",
        "    )(plus3)\n",
        "\n",
        "    stage2_unit2_relu1 = ReLU(name=\"stage2_unit2_relu1\")(stage2_unit2_bn1)\n",
        "\n",
        "    stage2_unit2_conv1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit2_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit2_relu1)\n",
        "\n",
        "    stage2_unit2_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn2\", trainable=False\n",
        "    )(stage2_unit2_conv1)\n",
        "\n",
        "    stage2_unit2_relu2 = ReLU(name=\"stage2_unit2_relu2\")(stage2_unit2_bn2)\n",
        "\n",
        "    stage2_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit2_relu2)\n",
        "\n",
        "    stage2_unit2_conv2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage2_unit2_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit2_conv2_pad)\n",
        "\n",
        "    stage2_unit2_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn3\", trainable=False\n",
        "    )(stage2_unit2_conv2)\n",
        "\n",
        "    stage2_unit2_relu3 = ReLU(name=\"stage2_unit2_relu3\")(stage2_unit2_bn3)\n",
        "\n",
        "    stage2_unit2_conv3 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit2_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit2_relu3)\n",
        "\n",
        "    plus4 = Add()([stage2_unit2_conv3, plus3])\n",
        "\n",
        "    stage2_unit3_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn1\", trainable=False\n",
        "    )(plus4)\n",
        "\n",
        "    stage2_unit3_relu1 = ReLU(name=\"stage2_unit3_relu1\")(stage2_unit3_bn1)\n",
        "\n",
        "    stage2_unit3_conv1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit3_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit3_relu1)\n",
        "\n",
        "    stage2_unit3_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn2\", trainable=False\n",
        "    )(stage2_unit3_conv1)\n",
        "\n",
        "    stage2_unit3_relu2 = ReLU(name=\"stage2_unit3_relu2\")(stage2_unit3_bn2)\n",
        "\n",
        "    stage2_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit3_relu2)\n",
        "\n",
        "    stage2_unit3_conv2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage2_unit3_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit3_conv2_pad)\n",
        "\n",
        "    stage2_unit3_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn3\", trainable=False\n",
        "    )(stage2_unit3_conv2)\n",
        "\n",
        "    stage2_unit3_relu3 = ReLU(name=\"stage2_unit3_relu3\")(stage2_unit3_bn3)\n",
        "\n",
        "    stage2_unit3_conv3 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit3_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit3_relu3)\n",
        "\n",
        "    plus5 = Add()([stage2_unit3_conv3, plus4])\n",
        "\n",
        "    stage2_unit4_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn1\", trainable=False\n",
        "    )(plus5)\n",
        "\n",
        "    stage2_unit4_relu1 = ReLU(name=\"stage2_unit4_relu1\")(stage2_unit4_bn1)\n",
        "\n",
        "    stage2_unit4_conv1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit4_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit4_relu1)\n",
        "\n",
        "    stage2_unit4_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn2\", trainable=False\n",
        "    )(stage2_unit4_conv1)\n",
        "\n",
        "    stage2_unit4_relu2 = ReLU(name=\"stage2_unit4_relu2\")(stage2_unit4_bn2)\n",
        "\n",
        "    stage2_unit4_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit4_relu2)\n",
        "\n",
        "    stage2_unit4_conv2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage2_unit4_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit4_conv2_pad)\n",
        "\n",
        "    stage2_unit4_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn3\", trainable=False\n",
        "    )(stage2_unit4_conv2)\n",
        "\n",
        "    stage2_unit4_relu3 = ReLU(name=\"stage2_unit4_relu3\")(stage2_unit4_bn3)\n",
        "\n",
        "    stage2_unit4_conv3 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage2_unit4_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage2_unit4_relu3)\n",
        "\n",
        "    plus6 = Add()([stage2_unit4_conv3, plus5])\n",
        "\n",
        "    stage3_unit1_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn1\", trainable=False\n",
        "    )(plus6)\n",
        "\n",
        "    stage3_unit1_relu1 = ReLU(name=\"stage3_unit1_relu1\")(stage3_unit1_bn1)\n",
        "\n",
        "    stage3_unit1_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit1_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit1_relu1)\n",
        "\n",
        "    stage3_unit1_sc = Conv2D(\n",
        "        filters=1024,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit1_sc\",\n",
        "        strides=[2, 2],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit1_relu1)\n",
        "\n",
        "    stage3_unit1_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn2\", trainable=False\n",
        "    )(stage3_unit1_conv1)\n",
        "\n",
        "    stage3_unit1_relu2 = ReLU(name=\"stage3_unit1_relu2\")(stage3_unit1_bn2)\n",
        "\n",
        "    stage3_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit1_relu2)\n",
        "\n",
        "    stage3_unit1_conv2 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage3_unit1_conv2\",\n",
        "        strides=[2, 2],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit1_conv2_pad)\n",
        "\n",
        "    ssh_m1_red_conv = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"ssh_m1_red_conv\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(stage3_unit1_relu2)\n",
        "\n",
        "    stage3_unit1_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn3\", trainable=False\n",
        "    )(stage3_unit1_conv2)\n",
        "\n",
        "    ssh_m1_red_conv_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_red_conv_bn\", trainable=False\n",
        "    )(ssh_m1_red_conv)\n",
        "\n",
        "    stage3_unit1_relu3 = ReLU(name=\"stage3_unit1_relu3\")(stage3_unit1_bn3)\n",
        "\n",
        "    ssh_m1_red_conv_relu = ReLU(name=\"ssh_m1_red_conv_relu\")(ssh_m1_red_conv_bn)\n",
        "\n",
        "    stage3_unit1_conv3 = Conv2D(\n",
        "        filters=1024,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit1_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit1_relu3)\n",
        "\n",
        "    plus7 = Add()([stage3_unit1_conv3, stage3_unit1_sc])\n",
        "\n",
        "    stage3_unit2_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn1\", trainable=False\n",
        "    )(plus7)\n",
        "\n",
        "    stage3_unit2_relu1 = ReLU(name=\"stage3_unit2_relu1\")(stage3_unit2_bn1)\n",
        "\n",
        "    stage3_unit2_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit2_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit2_relu1)\n",
        "\n",
        "    stage3_unit2_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn2\", trainable=False\n",
        "    )(stage3_unit2_conv1)\n",
        "\n",
        "    stage3_unit2_relu2 = ReLU(name=\"stage3_unit2_relu2\")(stage3_unit2_bn2)\n",
        "\n",
        "    stage3_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit2_relu2)\n",
        "\n",
        "    stage3_unit2_conv2 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage3_unit2_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit2_conv2_pad)\n",
        "\n",
        "    stage3_unit2_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn3\", trainable=False\n",
        "    )(stage3_unit2_conv2)\n",
        "\n",
        "    stage3_unit2_relu3 = ReLU(name=\"stage3_unit2_relu3\")(stage3_unit2_bn3)\n",
        "\n",
        "    stage3_unit2_conv3 = Conv2D(\n",
        "        filters=1024,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit2_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit2_relu3)\n",
        "\n",
        "    plus8 = Add()([stage3_unit2_conv3, plus7])\n",
        "\n",
        "    stage3_unit3_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn1\", trainable=False\n",
        "    )(plus8)\n",
        "\n",
        "    stage3_unit3_relu1 = ReLU(name=\"stage3_unit3_relu1\")(stage3_unit3_bn1)\n",
        "\n",
        "    stage3_unit3_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit3_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit3_relu1)\n",
        "\n",
        "    stage3_unit3_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn2\", trainable=False\n",
        "    )(stage3_unit3_conv1)\n",
        "\n",
        "    stage3_unit3_relu2 = ReLU(name=\"stage3_unit3_relu2\")(stage3_unit3_bn2)\n",
        "\n",
        "    stage3_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit3_relu2)\n",
        "\n",
        "    stage3_unit3_conv2 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage3_unit3_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit3_conv2_pad)\n",
        "\n",
        "    stage3_unit3_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn3\", trainable=False\n",
        "    )(stage3_unit3_conv2)\n",
        "\n",
        "    stage3_unit3_relu3 = ReLU(name=\"stage3_unit3_relu3\")(stage3_unit3_bn3)\n",
        "\n",
        "    stage3_unit3_conv3 = Conv2D(\n",
        "        filters=1024,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit3_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit3_relu3)\n",
        "\n",
        "    plus9 = Add()([stage3_unit3_conv3, plus8])\n",
        "\n",
        "    stage3_unit4_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn1\", trainable=False\n",
        "    )(plus9)\n",
        "\n",
        "    stage3_unit4_relu1 = ReLU(name=\"stage3_unit4_relu1\")(stage3_unit4_bn1)\n",
        "\n",
        "    stage3_unit4_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit4_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit4_relu1)\n",
        "\n",
        "    stage3_unit4_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn2\", trainable=False\n",
        "    )(stage3_unit4_conv1)\n",
        "\n",
        "    stage3_unit4_relu2 = ReLU(name=\"stage3_unit4_relu2\")(stage3_unit4_bn2)\n",
        "\n",
        "    stage3_unit4_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit4_relu2)\n",
        "\n",
        "    stage3_unit4_conv2 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage3_unit4_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit4_conv2_pad)\n",
        "\n",
        "    stage3_unit4_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn3\", trainable=False\n",
        "    )(stage3_unit4_conv2)\n",
        "\n",
        "    stage3_unit4_relu3 = ReLU(name=\"stage3_unit4_relu3\")(stage3_unit4_bn3)\n",
        "\n",
        "    stage3_unit4_conv3 = Conv2D(\n",
        "        filters=1024,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit4_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit4_relu3)\n",
        "\n",
        "    plus10 = Add()([stage3_unit4_conv3, plus9])\n",
        "\n",
        "    stage3_unit5_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn1\", trainable=False\n",
        "    )(plus10)\n",
        "\n",
        "    stage3_unit5_relu1 = ReLU(name=\"stage3_unit5_relu1\")(stage3_unit5_bn1)\n",
        "\n",
        "    stage3_unit5_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit5_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit5_relu1)\n",
        "\n",
        "    stage3_unit5_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn2\", trainable=False\n",
        "    )(stage3_unit5_conv1)\n",
        "\n",
        "    stage3_unit5_relu2 = ReLU(name=\"stage3_unit5_relu2\")(stage3_unit5_bn2)\n",
        "\n",
        "    stage3_unit5_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit5_relu2)\n",
        "\n",
        "    stage3_unit5_conv2 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage3_unit5_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit5_conv2_pad)\n",
        "\n",
        "    stage3_unit5_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn3\", trainable=False\n",
        "    )(stage3_unit5_conv2)\n",
        "\n",
        "    stage3_unit5_relu3 = ReLU(name=\"stage3_unit5_relu3\")(stage3_unit5_bn3)\n",
        "\n",
        "    stage3_unit5_conv3 = Conv2D(\n",
        "        filters=1024,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit5_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit5_relu3)\n",
        "\n",
        "    plus11 = Add()([stage3_unit5_conv3, plus10])\n",
        "\n",
        "    stage3_unit6_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn1\", trainable=False\n",
        "    )(plus11)\n",
        "\n",
        "    stage3_unit6_relu1 = ReLU(name=\"stage3_unit6_relu1\")(stage3_unit6_bn1)\n",
        "\n",
        "    stage3_unit6_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit6_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit6_relu1)\n",
        "\n",
        "    stage3_unit6_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn2\", trainable=False\n",
        "    )(stage3_unit6_conv1)\n",
        "\n",
        "    stage3_unit6_relu2 = ReLU(name=\"stage3_unit6_relu2\")(stage3_unit6_bn2)\n",
        "\n",
        "    stage3_unit6_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit6_relu2)\n",
        "\n",
        "    stage3_unit6_conv2 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage3_unit6_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit6_conv2_pad)\n",
        "\n",
        "    stage3_unit6_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn3\", trainable=False\n",
        "    )(stage3_unit6_conv2)\n",
        "\n",
        "    stage3_unit6_relu3 = ReLU(name=\"stage3_unit6_relu3\")(stage3_unit6_bn3)\n",
        "\n",
        "    stage3_unit6_conv3 = Conv2D(\n",
        "        filters=1024,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage3_unit6_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage3_unit6_relu3)\n",
        "\n",
        "    plus12 = Add()([stage3_unit6_conv3, plus11])\n",
        "\n",
        "    stage4_unit1_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn1\", trainable=False\n",
        "    )(plus12)\n",
        "\n",
        "    stage4_unit1_relu1 = ReLU(name=\"stage4_unit1_relu1\")(stage4_unit1_bn1)\n",
        "\n",
        "    stage4_unit1_conv1 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage4_unit1_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit1_relu1)\n",
        "\n",
        "    stage4_unit1_sc = Conv2D(\n",
        "        filters=2048,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage4_unit1_sc\",\n",
        "        strides=[2, 2],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit1_relu1)\n",
        "\n",
        "    stage4_unit1_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn2\", trainable=False\n",
        "    )(stage4_unit1_conv1)\n",
        "\n",
        "    stage4_unit1_relu2 = ReLU(name=\"stage4_unit1_relu2\")(stage4_unit1_bn2)\n",
        "\n",
        "    stage4_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit1_relu2)\n",
        "\n",
        "    stage4_unit1_conv2 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage4_unit1_conv2\",\n",
        "        strides=[2, 2],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit1_conv2_pad)\n",
        "\n",
        "    ssh_c2_lateral = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"ssh_c2_lateral\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(stage4_unit1_relu2)\n",
        "\n",
        "    stage4_unit1_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn3\", trainable=False\n",
        "    )(stage4_unit1_conv2)\n",
        "\n",
        "    ssh_c2_lateral_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_c2_lateral_bn\", trainable=False\n",
        "    )(ssh_c2_lateral)\n",
        "\n",
        "    stage4_unit1_relu3 = ReLU(name=\"stage4_unit1_relu3\")(stage4_unit1_bn3)\n",
        "\n",
        "    ssh_c2_lateral_relu = ReLU(name=\"ssh_c2_lateral_relu\")(ssh_c2_lateral_bn)\n",
        "\n",
        "    stage4_unit1_conv3 = Conv2D(\n",
        "        filters=2048,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage4_unit1_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit1_relu3)\n",
        "\n",
        "    plus13 = Add()([stage4_unit1_conv3, stage4_unit1_sc])\n",
        "\n",
        "    stage4_unit2_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn1\", trainable=False\n",
        "    )(plus13)\n",
        "\n",
        "    stage4_unit2_relu1 = ReLU(name=\"stage4_unit2_relu1\")(stage4_unit2_bn1)\n",
        "\n",
        "    stage4_unit2_conv1 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage4_unit2_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit2_relu1)\n",
        "\n",
        "    stage4_unit2_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn2\", trainable=False\n",
        "    )(stage4_unit2_conv1)\n",
        "\n",
        "    stage4_unit2_relu2 = ReLU(name=\"stage4_unit2_relu2\")(stage4_unit2_bn2)\n",
        "\n",
        "    stage4_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit2_relu2)\n",
        "\n",
        "    stage4_unit2_conv2 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage4_unit2_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit2_conv2_pad)\n",
        "\n",
        "    stage4_unit2_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn3\", trainable=False\n",
        "    )(stage4_unit2_conv2)\n",
        "\n",
        "    stage4_unit2_relu3 = ReLU(name=\"stage4_unit2_relu3\")(stage4_unit2_bn3)\n",
        "\n",
        "    stage4_unit2_conv3 = Conv2D(\n",
        "        filters=2048,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage4_unit2_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit2_relu3)\n",
        "\n",
        "    plus14 = Add()([stage4_unit2_conv3, plus13])\n",
        "\n",
        "    stage4_unit3_bn1 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn1\", trainable=False\n",
        "    )(plus14)\n",
        "\n",
        "    stage4_unit3_relu1 = ReLU(name=\"stage4_unit3_relu1\")(stage4_unit3_bn1)\n",
        "\n",
        "    stage4_unit3_conv1 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage4_unit3_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit3_relu1)\n",
        "\n",
        "    stage4_unit3_bn2 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn2\", trainable=False\n",
        "    )(stage4_unit3_conv1)\n",
        "\n",
        "    stage4_unit3_relu2 = ReLU(name=\"stage4_unit3_relu2\")(stage4_unit3_bn2)\n",
        "\n",
        "    stage4_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit3_relu2)\n",
        "\n",
        "    stage4_unit3_conv2 = Conv2D(\n",
        "        filters=512,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"stage4_unit3_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit3_conv2_pad)\n",
        "\n",
        "    stage4_unit3_bn3 = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn3\", trainable=False\n",
        "    )(stage4_unit3_conv2)\n",
        "\n",
        "    stage4_unit3_relu3 = ReLU(name=\"stage4_unit3_relu3\")(stage4_unit3_bn3)\n",
        "\n",
        "    stage4_unit3_conv3 = Conv2D(\n",
        "        filters=2048,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"stage4_unit3_conv3\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=False,\n",
        "    )(stage4_unit3_relu3)\n",
        "\n",
        "    plus15 = Add()([stage4_unit3_conv3, plus14])\n",
        "\n",
        "    bn1 = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn1\", trainable=False)(plus15)\n",
        "\n",
        "    relu1 = ReLU(name=\"relu1\")(bn1)\n",
        "\n",
        "    ssh_c3_lateral = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"ssh_c3_lateral\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(relu1)\n",
        "\n",
        "    ssh_c3_lateral_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_c3_lateral_bn\", trainable=False\n",
        "    )(ssh_c3_lateral)\n",
        "\n",
        "    ssh_c3_lateral_relu = ReLU(name=\"ssh_c3_lateral_relu\")(ssh_c3_lateral_bn)\n",
        "\n",
        "    ssh_m3_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c3_lateral_relu)\n",
        "\n",
        "    ssh_m3_det_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m3_det_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_conv1_pad)\n",
        "\n",
        "    ssh_m3_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c3_lateral_relu)\n",
        "\n",
        "    ssh_m3_det_context_conv1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m3_det_context_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_context_conv1_pad)\n",
        "\n",
        "    ssh_c3_up = UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=\"ssh_c3_up\")(\n",
        "        ssh_c3_lateral_relu\n",
        "    )\n",
        "\n",
        "    ssh_m3_det_conv1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_conv1_bn\", trainable=False\n",
        "    )(ssh_m3_det_conv1)\n",
        "\n",
        "    ssh_m3_det_context_conv1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv1_bn\", trainable=False\n",
        "    )(ssh_m3_det_context_conv1)\n",
        "\n",
        "    x1_shape = tf.shape(ssh_c3_up)\n",
        "    x2_shape = tf.shape(ssh_c2_lateral_relu)\n",
        "    offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
        "    size = [-1, x2_shape[1], x2_shape[2], -1]\n",
        "    crop0 = tf.slice(ssh_c3_up, offsets, size, \"crop0\")\n",
        "\n",
        "    ssh_m3_det_context_conv1_relu = ReLU(name=\"ssh_m3_det_context_conv1_relu\")(\n",
        "        ssh_m3_det_context_conv1_bn\n",
        "    )\n",
        "\n",
        "    plus0_v2 = Add()([ssh_c2_lateral_relu, crop0])\n",
        "\n",
        "    ssh_m3_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m3_det_context_conv1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m3_det_context_conv2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m3_det_context_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_context_conv2_pad)\n",
        "\n",
        "    ssh_m3_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m3_det_context_conv1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m3_det_context_conv3_1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m3_det_context_conv3_1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_context_conv3_1_pad)\n",
        "\n",
        "    ssh_c2_aggr_pad = ZeroPadding2D(padding=tuple([1, 1]))(plus0_v2)\n",
        "\n",
        "    ssh_c2_aggr = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_c2_aggr\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_c2_aggr_pad)\n",
        "\n",
        "    ssh_m3_det_context_conv2_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv2_bn\", trainable=False\n",
        "    )(ssh_m3_det_context_conv2)\n",
        "\n",
        "    ssh_m3_det_context_conv3_1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv3_1_bn\", trainable=False\n",
        "    )(ssh_m3_det_context_conv3_1)\n",
        "\n",
        "    ssh_c2_aggr_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_c2_aggr_bn\", trainable=False\n",
        "    )(ssh_c2_aggr)\n",
        "\n",
        "    ssh_m3_det_context_conv3_1_relu = ReLU(name=\"ssh_m3_det_context_conv3_1_relu\")(\n",
        "        ssh_m3_det_context_conv3_1_bn\n",
        "    )\n",
        "\n",
        "    ssh_c2_aggr_relu = ReLU(name=\"ssh_c2_aggr_relu\")(ssh_c2_aggr_bn)\n",
        "\n",
        "    ssh_m3_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m3_det_context_conv3_1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m3_det_context_conv3_2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m3_det_context_conv3_2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_context_conv3_2_pad)\n",
        "\n",
        "    ssh_m2_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c2_aggr_relu)\n",
        "\n",
        "    ssh_m2_det_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m2_det_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_conv1_pad)\n",
        "\n",
        "    ssh_m2_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c2_aggr_relu)\n",
        "\n",
        "    ssh_m2_det_context_conv1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m2_det_context_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_context_conv1_pad)\n",
        "\n",
        "    ssh_m2_red_up = UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=\"ssh_m2_red_up\")(\n",
        "        ssh_c2_aggr_relu\n",
        "    )\n",
        "\n",
        "    ssh_m3_det_context_conv3_2_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv3_2_bn\", trainable=False\n",
        "    )(ssh_m3_det_context_conv3_2)\n",
        "\n",
        "    ssh_m2_det_conv1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_conv1_bn\", trainable=False\n",
        "    )(ssh_m2_det_conv1)\n",
        "\n",
        "    ssh_m2_det_context_conv1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv1_bn\", trainable=False\n",
        "    )(ssh_m2_det_context_conv1)\n",
        "\n",
        "    x1_shape = tf.shape(ssh_m2_red_up)\n",
        "    x2_shape = tf.shape(ssh_m1_red_conv_relu)\n",
        "    offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
        "    size = [-1, x2_shape[1], x2_shape[2], -1]\n",
        "    crop1 = tf.slice(ssh_m2_red_up, offsets, size, \"crop1\")\n",
        "\n",
        "    ssh_m3_det_concat = concatenate(\n",
        "        [ssh_m3_det_conv1_bn, ssh_m3_det_context_conv2_bn, ssh_m3_det_context_conv3_2_bn],\n",
        "        3,\n",
        "        name=\"ssh_m3_det_concat\",\n",
        "    )\n",
        "\n",
        "    ssh_m2_det_context_conv1_relu = ReLU(name=\"ssh_m2_det_context_conv1_relu\")(\n",
        "        ssh_m2_det_context_conv1_bn\n",
        "    )\n",
        "\n",
        "    plus1_v1 = Add()([ssh_m1_red_conv_relu, crop1])\n",
        "\n",
        "    ssh_m3_det_concat_relu = ReLU(name=\"ssh_m3_det_concat_relu\")(ssh_m3_det_concat)\n",
        "\n",
        "    ssh_m2_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m2_det_context_conv1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m2_det_context_conv2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m2_det_context_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_context_conv2_pad)\n",
        "\n",
        "    ssh_m2_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m2_det_context_conv1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m2_det_context_conv3_1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m2_det_context_conv3_1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_context_conv3_1_pad)\n",
        "\n",
        "    ssh_c1_aggr_pad = ZeroPadding2D(padding=tuple([1, 1]))(plus1_v1)\n",
        "\n",
        "    ssh_c1_aggr = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_c1_aggr\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_c1_aggr_pad)\n",
        "\n",
        "    face_rpn_cls_score_stride32 = Conv2D(\n",
        "        filters=4,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_cls_score_stride32\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_concat_relu)\n",
        "\n",
        "    inter_1 = concatenate(\n",
        "        [face_rpn_cls_score_stride32[:, :, :, 0], face_rpn_cls_score_stride32[:, :, :, 1]], axis=1\n",
        "    )\n",
        "    inter_2 = concatenate(\n",
        "        [face_rpn_cls_score_stride32[:, :, :, 2], face_rpn_cls_score_stride32[:, :, :, 3]], axis=1\n",
        "    )\n",
        "    final = tf.stack([inter_1, inter_2])\n",
        "    face_rpn_cls_score_reshape_stride32 = tf.transpose(\n",
        "        final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride32\"\n",
        "    )\n",
        "\n",
        "    face_rpn_bbox_pred_stride32 = Conv2D(\n",
        "        filters=8,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_bbox_pred_stride32\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_concat_relu)\n",
        "\n",
        "    face_rpn_landmark_pred_stride32 = Conv2D(\n",
        "        filters=20,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_landmark_pred_stride32\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m3_det_concat_relu)\n",
        "\n",
        "    ssh_m2_det_context_conv2_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv2_bn\", trainable=False\n",
        "    )(ssh_m2_det_context_conv2)\n",
        "\n",
        "    ssh_m2_det_context_conv3_1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv3_1_bn\", trainable=False\n",
        "    )(ssh_m2_det_context_conv3_1)\n",
        "\n",
        "    ssh_c1_aggr_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_c1_aggr_bn\", trainable=False\n",
        "    )(ssh_c1_aggr)\n",
        "\n",
        "    ssh_m2_det_context_conv3_1_relu = ReLU(name=\"ssh_m2_det_context_conv3_1_relu\")(\n",
        "        ssh_m2_det_context_conv3_1_bn\n",
        "    )\n",
        "\n",
        "    ssh_c1_aggr_relu = ReLU(name=\"ssh_c1_aggr_relu\")(ssh_c1_aggr_bn)\n",
        "\n",
        "    face_rpn_cls_prob_stride32 = Softmax(name=\"face_rpn_cls_prob_stride32\")(\n",
        "        face_rpn_cls_score_reshape_stride32\n",
        "    )\n",
        "\n",
        "    input_shape = [tf.shape(face_rpn_cls_prob_stride32)[k] for k in range(4)]\n",
        "    sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
        "    inter_1 = face_rpn_cls_prob_stride32[:, 0:sz, :, 0]\n",
        "    inter_2 = face_rpn_cls_prob_stride32[:, 0:sz, :, 1]\n",
        "    inter_3 = face_rpn_cls_prob_stride32[:, sz:, :, 0]\n",
        "    inter_4 = face_rpn_cls_prob_stride32[:, sz:, :, 1]\n",
        "    final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
        "    face_rpn_cls_prob_reshape_stride32 = tf.transpose(\n",
        "        final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride32\"\n",
        "    )\n",
        "\n",
        "    ssh_m2_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m2_det_context_conv3_1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m2_det_context_conv3_2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m2_det_context_conv3_2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_context_conv3_2_pad)\n",
        "\n",
        "    ssh_m1_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c1_aggr_relu)\n",
        "\n",
        "    ssh_m1_det_conv1 = Conv2D(\n",
        "        filters=256,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m1_det_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_conv1_pad)\n",
        "\n",
        "    ssh_m1_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c1_aggr_relu)\n",
        "\n",
        "    ssh_m1_det_context_conv1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m1_det_context_conv1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_context_conv1_pad)\n",
        "\n",
        "    ssh_m2_det_context_conv3_2_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv3_2_bn\", trainable=False\n",
        "    )(ssh_m2_det_context_conv3_2)\n",
        "\n",
        "    ssh_m1_det_conv1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_conv1_bn\", trainable=False\n",
        "    )(ssh_m1_det_conv1)\n",
        "\n",
        "    ssh_m1_det_context_conv1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv1_bn\", trainable=False\n",
        "    )(ssh_m1_det_context_conv1)\n",
        "\n",
        "    ssh_m2_det_concat = concatenate(\n",
        "        [ssh_m2_det_conv1_bn, ssh_m2_det_context_conv2_bn, ssh_m2_det_context_conv3_2_bn],\n",
        "        3,\n",
        "        name=\"ssh_m2_det_concat\",\n",
        "    )\n",
        "\n",
        "    ssh_m1_det_context_conv1_relu = ReLU(name=\"ssh_m1_det_context_conv1_relu\")(\n",
        "        ssh_m1_det_context_conv1_bn\n",
        "    )\n",
        "\n",
        "    ssh_m2_det_concat_relu = ReLU(name=\"ssh_m2_det_concat_relu\")(ssh_m2_det_concat)\n",
        "\n",
        "    ssh_m1_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m1_det_context_conv1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m1_det_context_conv2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m1_det_context_conv2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_context_conv2_pad)\n",
        "\n",
        "    ssh_m1_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m1_det_context_conv1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m1_det_context_conv3_1 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m1_det_context_conv3_1\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_context_conv3_1_pad)\n",
        "\n",
        "    face_rpn_cls_score_stride16 = Conv2D(\n",
        "        filters=4,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_cls_score_stride16\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_concat_relu)\n",
        "\n",
        "    inter_1 = concatenate(\n",
        "        [face_rpn_cls_score_stride16[:, :, :, 0], face_rpn_cls_score_stride16[:, :, :, 1]], axis=1\n",
        "    )\n",
        "    inter_2 = concatenate(\n",
        "        [face_rpn_cls_score_stride16[:, :, :, 2], face_rpn_cls_score_stride16[:, :, :, 3]], axis=1\n",
        "    )\n",
        "    final = tf.stack([inter_1, inter_2])\n",
        "    face_rpn_cls_score_reshape_stride16 = tf.transpose(\n",
        "        final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride16\"\n",
        "    )\n",
        "\n",
        "    face_rpn_bbox_pred_stride16 = Conv2D(\n",
        "        filters=8,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_bbox_pred_stride16\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_concat_relu)\n",
        "\n",
        "    face_rpn_landmark_pred_stride16 = Conv2D(\n",
        "        filters=20,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_landmark_pred_stride16\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m2_det_concat_relu)\n",
        "\n",
        "    ssh_m1_det_context_conv2_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv2_bn\", trainable=False\n",
        "    )(ssh_m1_det_context_conv2)\n",
        "\n",
        "    ssh_m1_det_context_conv3_1_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv3_1_bn\", trainable=False\n",
        "    )(ssh_m1_det_context_conv3_1)\n",
        "\n",
        "    ssh_m1_det_context_conv3_1_relu = ReLU(name=\"ssh_m1_det_context_conv3_1_relu\")(\n",
        "        ssh_m1_det_context_conv3_1_bn\n",
        "    )\n",
        "\n",
        "    face_rpn_cls_prob_stride16 = Softmax(name=\"face_rpn_cls_prob_stride16\")(\n",
        "        face_rpn_cls_score_reshape_stride16\n",
        "    )\n",
        "\n",
        "    input_shape = [tf.shape(face_rpn_cls_prob_stride16)[k] for k in range(4)]\n",
        "    sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
        "    inter_1 = face_rpn_cls_prob_stride16[:, 0:sz, :, 0]\n",
        "    inter_2 = face_rpn_cls_prob_stride16[:, 0:sz, :, 1]\n",
        "    inter_3 = face_rpn_cls_prob_stride16[:, sz:, :, 0]\n",
        "    inter_4 = face_rpn_cls_prob_stride16[:, sz:, :, 1]\n",
        "    final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
        "    face_rpn_cls_prob_reshape_stride16 = tf.transpose(\n",
        "        final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride16\"\n",
        "    )\n",
        "\n",
        "    ssh_m1_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "        ssh_m1_det_context_conv3_1_relu\n",
        "    )\n",
        "\n",
        "    ssh_m1_det_context_conv3_2 = Conv2D(\n",
        "        filters=128,\n",
        "        kernel_size=(3, 3),\n",
        "        name=\"ssh_m1_det_context_conv3_2\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_context_conv3_2_pad)\n",
        "\n",
        "    ssh_m1_det_context_conv3_2_bn = BatchNormalization(\n",
        "        epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv3_2_bn\", trainable=False\n",
        "    )(ssh_m1_det_context_conv3_2)\n",
        "\n",
        "    ssh_m1_det_concat = concatenate(\n",
        "        [ssh_m1_det_conv1_bn, ssh_m1_det_context_conv2_bn, ssh_m1_det_context_conv3_2_bn],\n",
        "        3,\n",
        "        name=\"ssh_m1_det_concat\",\n",
        "    )\n",
        "\n",
        "    ssh_m1_det_concat_relu = ReLU(name=\"ssh_m1_det_concat_relu\")(ssh_m1_det_concat)\n",
        "    face_rpn_cls_score_stride8 = Conv2D(\n",
        "        filters=4,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_cls_score_stride8\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_concat_relu)\n",
        "\n",
        "    inter_1 = concatenate(\n",
        "        [face_rpn_cls_score_stride8[:, :, :, 0], face_rpn_cls_score_stride8[:, :, :, 1]], axis=1\n",
        "    )\n",
        "    inter_2 = concatenate(\n",
        "        [face_rpn_cls_score_stride8[:, :, :, 2], face_rpn_cls_score_stride8[:, :, :, 3]], axis=1\n",
        "    )\n",
        "    final = tf.stack([inter_1, inter_2])\n",
        "    face_rpn_cls_score_reshape_stride8 = tf.transpose(\n",
        "        final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride8\"\n",
        "    )\n",
        "\n",
        "    face_rpn_bbox_pred_stride8 = Conv2D(\n",
        "        filters=8,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_bbox_pred_stride8\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_concat_relu)\n",
        "\n",
        "    face_rpn_landmark_pred_stride8 = Conv2D(\n",
        "        filters=20,\n",
        "        kernel_size=(1, 1),\n",
        "        name=\"face_rpn_landmark_pred_stride8\",\n",
        "        strides=[1, 1],\n",
        "        padding=\"VALID\",\n",
        "        use_bias=True,\n",
        "    )(ssh_m1_det_concat_relu)\n",
        "\n",
        "    face_rpn_cls_prob_stride8 = Softmax(name=\"face_rpn_cls_prob_stride8\")(\n",
        "        face_rpn_cls_score_reshape_stride8\n",
        "    )\n",
        "\n",
        "    input_shape = [tf.shape(face_rpn_cls_prob_stride8)[k] for k in range(4)]\n",
        "    sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
        "    inter_1 = face_rpn_cls_prob_stride8[:, 0:sz, :, 0]\n",
        "    inter_2 = face_rpn_cls_prob_stride8[:, 0:sz, :, 1]\n",
        "    inter_3 = face_rpn_cls_prob_stride8[:, sz:, :, 0]\n",
        "    inter_4 = face_rpn_cls_prob_stride8[:, sz:, :, 1]\n",
        "    final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
        "    face_rpn_cls_prob_reshape_stride8 = tf.transpose(\n",
        "        final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride8\"\n",
        "    )\n",
        "\n",
        "    model = Model(\n",
        "        inputs=data,\n",
        "        outputs=[\n",
        "            face_rpn_cls_prob_reshape_stride32,\n",
        "            face_rpn_bbox_pred_stride32,\n",
        "            face_rpn_landmark_pred_stride32,\n",
        "            face_rpn_cls_prob_reshape_stride16,\n",
        "            face_rpn_bbox_pred_stride16,\n",
        "            face_rpn_landmark_pred_stride16,\n",
        "            face_rpn_cls_prob_reshape_stride8,\n",
        "            face_rpn_bbox_pred_stride8,\n",
        "            face_rpn_landmark_pred_stride8,\n",
        "        ],\n",
        "    )\n",
        "    model.load_weights(exact_file)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "bw4f04YgtvtG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=build_model('/content/retinaface.h5')"
      ],
      "metadata": {
        "id": "DKt3nMYTvHMA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "KY6yBeQavNdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "model.save('retina_saved_model')"
      ],
      "metadata": {
        "id": "BbgUHaq-q7GK",
        "outputId": "baac933c-e928-432e-bc75-f5ac8c4ef884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('retina_saved_model')"
      ],
      "metadata": {
        "id": "Xhg2y3vJq7I2",
        "outputId": "95e1a90a-3fae-4d6f-8b1c-c73c1a4386ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./data\n",
        "!wget  -O ./data/img0.JPG \"https://d17fnq9dkz9hgj.cloudfront.net/breed-uploads/2018/08/siberian-husky-detail.jpg?bust=1535566590&width=630\"\n",
        "!wget  -O ./data/img1.JPG \"https://www.hakaimagazine.com/wp-content/uploads/header-gulf-birds.jpg\"\n",
        "!wget  -O ./data/img2.JPG \"https://www.artis.nl/media/filer_public_thumbnails/filer_public/00/f1/00f1b6db-fbed-4fef-9ab0-84e944ff11f8/chimpansee_amber_r_1920x1080.jpg__1920x1080_q85_subject_location-923%2C365_subsampling-2.jpg\"\n",
        "!wget  -O ./data/img3.JPG \"https://www.familyhandyman.com/wp-content/uploads/2018/09/How-to-Avoid-Snakes-Slithering-Up-Your-Toilet-shutterstock_780480850.jpg\""
      ],
      "metadata": {
        "id": "aq4g8eUkq7LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Converting to TF-TRT FP32...')\n",
        "conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt.TrtPrecisionMode.FP32,\n",
        "                                                               max_workspace_size_bytes=8000000000)\n",
        "\n",
        "converter = trt.TrtGraphConverterV2(input_saved_model_dir='retina_saved_model',\n",
        "                                    conversion_params=conversion_params)\n",
        "converter.convert()\n",
        "converter.save(output_saved_model_dir='retina_saved_model_TFTRT_FP32')\n",
        "print('Done Converting to TF-TRT FP32')"
      ],
      "metadata": {
        "id": "M3wfi7vTq7N6",
        "outputId": "ed0b9bbf-b117-4f1d-9e83-6440aec0ad49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting to TF-TRT FP32...\n",
            "Done Converting to TF-TRT FP32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "batched_input = np.zeros((batch_size, 512, 512, 3), dtype=np.float32)\n",
        "\n",
        "for i in range(batch_size):\n",
        "  img_path = './data/img%d.JPG' % (i % 4)\n",
        "  img = image.load_img(img_path, target_size=(512, 512))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "  batched_input[i, :] = x\n",
        "batched_input = tf.constant(batched_input)\n",
        "print('batched_input shape: ', batched_input.shape)"
      ],
      "metadata": {
        "id": "SXlonNuKr2Zk",
        "outputId": "b5981a85-2238-4274-f83e-3ee00bd82cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batched_input shape:  (8, 512, 512, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load model\n",
        "trt_model=tf.saved_model.load('/content/retina_saved_model_TFTRT_FP32')\n",
        "inference=trt_model.signatures['serving_default']"
      ],
      "metadata": {
        "id": "KtV8IyPXwPO-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Size 1024x1980 batch 8"
      ],
      "metadata": {
        "id": "bt1Kq64JysiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "inference(batched_input)\n",
        "end_time = time.time()\n",
        "gap=(end_time-start_time)*1000\n",
        "#elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "print('total time : {}ms'.format(gap ))\n",
        "#print('total time : {:4.1f}ms'.format( (elapsed_time[-50:].mean()) * 1000))"
      ],
      "metadata": {
        "id": "tSyrUsrJr2cI",
        "outputId": "dfa80251-3627-4eac-e0c9-0a5b4f147438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total time : 982.8886985778809ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Size 512x512 batch 8"
      ],
      "metadata": {
        "id": "CBkaucGNym5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "inference(batched_input)\n",
        "end_time = time.time()\n",
        "gap=(end_time-start_time)*1000\n",
        "#elapsed_time = np.append(elapsed_time, end_time - start_time)\n",
        "print('total time : {}ms'.format(gap ))\n",
        "#print('total time : {:4.1f}ms'.format( (elapsed_time[-50:].mean()) * 1000))"
      ],
      "metadata": {
        "id": "nxZO9kQMr2ee",
        "outputId": "a38a9f35-bc32-4f50-ed6f-6e3c1ccd6dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total time : 120.7437515258789ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U1Qbdt1Ar2g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hef5zatq7QM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}