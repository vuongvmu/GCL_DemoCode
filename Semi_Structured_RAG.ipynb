{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuongvmu/GCL_DemoCode/blob/main/Semi_Structured_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d466cc-aa8b-4baf-a80a-fef01921ca8d",
      "metadata": {
        "id": "b6d466cc-aa8b-4baf-a80a-fef01921ca8d"
      },
      "source": [
        "## Semi-structured RAG\n",
        "\n",
        "Many documents contain a mixture of content types, including text and tables.\n",
        "\n",
        "Semi-structured data can be challenging for conventional RAG for at least two reasons:\n",
        "\n",
        "* Text splitting may break up tables, corrupting the data in retrieval\n",
        "* Embedding tables may pose challenges for semantic similarity search\n",
        "\n",
        "This cookbook shows how to perform RAG on documents with semi-structured data:\n",
        "\n",
        "\n",
        "The overall flow is here:\n",
        "\n",
        "\n",
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5740fc70-c513-4ff4-9d72-cfc098f85fef",
      "metadata": {
        "id": "5740fc70-c513-4ff4-9d72-cfc098f85fef"
      },
      "outputs": [],
      "source": [
        "! pip install langchain unstructured[all-docs] pydantic lxml langchainhub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44349a83-e1dc-4eed-ba75-587f309d8c88",
      "metadata": {
        "id": "44349a83-e1dc-4eed-ba75-587f309d8c88"
      },
      "source": [
        "The PDF partitioning used by Unstructured will use:\n",
        "\n",
        "* `tesseract` for Optical Character Recognition (OCR)\n",
        "*  `poppler` for PDF rendering and processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pN2obAMiJf3",
        "outputId": "2f0eeaa1-74f5-4f96-f3c5-7604e56a80b3"
      },
      "id": "9pN2obAMiJf3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE1dsfKAmGen",
        "outputId": "63ea1b8f-2c67-4c50-f12a-6341f8a40a0f"
      },
      "id": "oE1dsfKAmGen",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7880871-4949-4ea2-aed8-540a09188a41",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7880871-4949-4ea2-aed8-540a09188a41",
        "outputId": "0b5a0227-f163-4f93-bb0e-df198ef08b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.2.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pytesseract\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50eS3eNgpqHG",
        "outputId": "caa7e10e-2ff3-460e-e379-f90104efc3dd"
      },
      "id": "50eS3eNgpqHG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c24efa9-b6f6-4dc2-bfe3-70819ba3ef75",
      "metadata": {
        "id": "7c24efa9-b6f6-4dc2-bfe3-70819ba3ef75"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "### Partition PDF tables and text\n",
        "\n",
        "Apply to the [`Gemini`](https://arxiv.org/abs/2312.11805) paper.\n",
        "\n",
        "We use the Unstructured [`partition_pdf`](https://unstructured-io.github.io/unstructured/bricks/partition.html#partition-pdf), which segments a PDF document by using a layout model.\n",
        "\n",
        "This layout model makes it possible to extract elements, such as tables, from pdfs.\n",
        "\n",
        "We also can use `Unstructured` chunking, which:\n",
        "\n",
        "* Tries to identify document sections (e.g., Introduction, etc)\n",
        "* Then, builds text blocks that maintain sections while also honoring user-defined chunk sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62cf502b-407d-4645-a72c-24498fd55130",
      "metadata": {
        "id": "62cf502b-407d-4645-a72c-24498fd55130"
      },
      "outputs": [],
      "source": [
        "path = \"/content/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3867a654-61ba-4759-9a64-de953a429ced",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3867a654-61ba-4759-9a64-de953a429ced",
        "outputId": "d97c9f72-f215-4a1f-b0b7-27e4d0948e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked']\n",
            "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import pytesseract\n",
        "import nltk\n",
        "import nltk.internals\n",
        "nltk.download('punkt')\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "pytesseract.pytesseract.tesseract_cmd = ( r'/usr/bin/tesseract' )\n",
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=path + \"Gemini.pdf\",\n",
        "    # Unstructured first finds embedded image blocks\n",
        "    extract_images_in_pdf=False,\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b09cd727-aeab-49af-8a51-0dc377321e7c",
      "metadata": {
        "id": "b09cd727-aeab-49af-8a51-0dc377321e7c"
      },
      "source": [
        "We can examine the elements extracted by `partition_pdf`.\n",
        "\n",
        "`CompositeElement` are aggregated chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "628abfc6-4057-434b-b880-d88e3ba44657",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "628abfc6-4057-434b-b880-d88e3ba44657",
        "outputId": "4aa6d9e2-4e3a-4214-fd4f-f07bd749af5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"<class 'unstructured.documents.elements.CompositeElement'>\": 55,\n",
              " \"<class 'unstructured.documents.elements.Table'>\": 16}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Create a dictionary to store counts of each type\n",
        "category_counts = {}\n",
        "\n",
        "for element in raw_pdf_elements:\n",
        "    category = str(type(element))\n",
        "    if category in category_counts:\n",
        "        category_counts[category] += 1\n",
        "    else:\n",
        "        category_counts[category] = 1\n",
        "\n",
        "# Unique_categories will have unique elements\n",
        "unique_categories = set(category_counts.keys())\n",
        "category_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5462f29e-fd59-4e0e-9493-ea3b560e523e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5462f29e-fd59-4e0e-9493-ea3b560e523e",
        "outputId": "ecc9c803-ab8b-4404-841c-684b16ca9cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n",
            "55\n"
          ]
        }
      ],
      "source": [
        "class Element(BaseModel):\n",
        "    type: str\n",
        "    text: Any\n",
        "\n",
        "\n",
        "# Categorize by type\n",
        "categorized_elements = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
        "\n",
        "# Tables\n",
        "table_elements = [e for e in categorized_elements if e.type == \"table\"]\n",
        "print(len(table_elements))\n",
        "\n",
        "# Text\n",
        "text_elements = [e for e in categorized_elements if e.type == \"text\"]\n",
        "print(len(text_elements))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731b3dfc-7ddf-4a11-9a30-9a79b7c66e16",
      "metadata": {
        "id": "731b3dfc-7ddf-4a11-9a30-9a79b7c66e16"
      },
      "source": [
        "## Multi-vector retriever\n",
        "\n",
        "Use [multi-vector-retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary) to produce summaries of tables and, optionally, text.\n",
        "\n",
        "With the summary, we will also store the raw table elements.\n",
        "\n",
        "The summaries are used to improve the quality of retrieval, [as explained in the multi vector retriever docs](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector).\n",
        "\n",
        "The raw tables are passed to the LLM, providing the full table context for the LLM to generate the answer.  \n",
        "\n",
        "### Summaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "T966JxwStggF"
      },
      "id": "T966JxwStggF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'OPENAI_API_KEY'"
      ],
      "metadata": {
        "id": "YzIiEOX0sFwa"
      },
      "id": "YzIiEOX0sFwa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e275736-3408-4d7a-990e-4362c88e81f8",
      "metadata": {
        "id": "8e275736-3408-4d7a-990e-4362c88e81f8"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b65677-aeb4-44fd-b06d-4539341ede97",
      "metadata": {
        "id": "37b65677-aeb4-44fd-b06d-4539341ede97"
      },
      "source": [
        "We create a simple summarize chain for each element.\n",
        "\n",
        "You can also see, re-use, or modify the prompt in the Hub [here](https://smith.langchain.com/hub/rlm/multi-vector-retriever-summarization).\n",
        "\n",
        "```\n",
        "from langchain import hub\n",
        "obj = hub.pull(\"rlm/multi-vector-retriever-summarization\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b12536a-1303-41ad-9948-4eb5a5f32614",
      "metadata": {
        "id": "1b12536a-1303-41ad-9948-4eb5a5f32614"
      },
      "outputs": [],
      "source": [
        "# Prompt\n",
        "prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text. \\\n",
        "Give a concise summary of the table or text. Table or text chunk: {element} \"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
        "\n",
        "# Summary chain\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d8b567c-b442-4bf0-b639-04bd89effc62",
      "metadata": {
        "id": "8d8b567c-b442-4bf0-b639-04bd89effc62"
      },
      "outputs": [],
      "source": [
        "# Apply to tables\n",
        "tables = [i.text for i in table_elements]\n",
        "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9c176c-3d46-4034-b169-0d7305d42d27",
      "metadata": {
        "id": "3e9c176c-3d46-4034-b169-0d7305d42d27"
      },
      "outputs": [],
      "source": [
        "# Apply to texts\n",
        "texts = [i.text for i in text_elements]\n",
        "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60524010-754f-4924-ad75-78cb54ca7257",
      "metadata": {
        "id": "60524010-754f-4924-ad75-78cb54ca7257"
      },
      "source": [
        "### Add to vectorstore\n",
        "\n",
        "Use [Multi Vector Retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#summary) with summaries:\n",
        "\n",
        "* `InMemoryStore` stores the raw text, tables\n",
        "* `vectorstore` stores the embedded summaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "rPdZgnANvd4T"
      },
      "id": "rPdZgnANvd4T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "346c3a02-8fea-4f75-a69e-fc9542b99dbc",
      "metadata": {
        "id": "346c3a02-8fea-4f75-a69e-fc9542b99dbc"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryStore()\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "\n",
        "# Add texts\n",
        "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
        "summary_texts = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(text_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_texts)\n",
        "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
        "\n",
        "# Add tables\n",
        "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
        "summary_tables = [\n",
        "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
        "    for i, s in enumerate(table_summaries)\n",
        "]\n",
        "retriever.vectorstore.add_documents(summary_tables)\n",
        "retriever.docstore.mset(list(zip(table_ids, tables)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8bbbd9-009b-4b34-a206-5874a60adbda",
      "metadata": {
        "id": "1d8bbbd9-009b-4b34-a206-5874a60adbda"
      },
      "source": [
        "## RAG\n",
        "\n",
        "Run [RAG pipeline](https://python.langchain.com/docs/expression_language/cookbook/retrieval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2489de4-51e3-48b4-bbcd-ed9171deadf3",
      "metadata": {
        "id": "f2489de4-51e3-48b4-bbcd-ed9171deadf3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Prompt template\n",
        "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# LLM\n",
        "model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "# RAG pipeline\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e3d100-10e8-4ee6-ae46-2480b1524ec8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "90e3d100-10e8-4ee6-ae46-2480b1524ec8",
        "outputId": "64c2a469-79ab-4b45-eb18-a8ef189cf99b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The performance of Gemini Ultra on the MMMU benchmark per discipline as per Table 8 is as follows:\\n\\n- Art & Design: 74.2\\n- Business: 62.7\\n- Science: 49.3\\n- Health & Medicine: 71.3\\n- Humanities & Social Science: 78.3\\n- Technology & Engineering: 53.0\\n\\nThe overall score is 62.4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "chain.invoke(\"What is the performance of Gemini Ultra performance on the MMMU benchmark per discipline as per Table 8?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"Give an overview of of the Gemini 1.0 model family\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "auFApAMUxDco",
        "outputId": "5974ee55-4497-40a9-a757-98d632e9543f"
      },
      "id": "auFApAMUxDco",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Gemini 1.0 model family is a new family of multimodal models introduced by Google DeepMind. The family consists of Ultra, Pro, and Nano sizes, each suitable for different applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. The models are evaluated on a broad range of benchmarks, with the most capable model, Gemini Ultra, advancing the state of the art in 30 of 32 of these benchmarks. Notably, Gemini Ultra is the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and it improves the state of the art in every one of the 20 multimodal benchmarks examined. The Gemini models exhibit remarkable capabilities across image, audio, video, and text understanding. They are designed to enable a wide variety of use cases and are being responsibly deployed to users.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(\"What are the results of Automatic speech recognition taks on Youtube\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nW1KN4DrxRLi",
        "outputId": "7e7e7513-0e26-4d1e-adb3-4f82b2d19ffe"
      },
      "id": "nW1KN4DrxRLi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The results of Automatic Speech Recognition tasks on YouTube are as follows: Gemini Pro had a Word Error Rate (WER) of 4.9%, Gemini Nano-1 had a WER of 5.5%, and Whisper had a WER of 6.5% for v3 and 6.2% for v2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}