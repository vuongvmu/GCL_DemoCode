{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vuongvmu/GCL_DemoCode/blob/main/rentina.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creat memory"
      ],
      "metadata": {
        "id": "nwbFQhYVBx1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "\n",
        "from retinaface import RetinaFace\n",
        "import cv2\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
        "    pass\n",
        "else:\n",
        "    # Handle target environment that doesn't support HTTPS verification\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "\n",
        "from FDetector import FDetector\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# configurations\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "# Limit the amount of reserved VRAM so that other scripts can be run in the same GPU as well\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "\n",
        "img1 = cv2.imread('d://222.png')\n",
        "img2 = cv2.imread(\"d://111.png\")\n",
        "img1 = cv2.resize(img1, (640,480))\n",
        "img2 = cv2.resize(img2, (640,480))\n",
        "\n",
        "model = FDetector()\n",
        "img = np.stack([img1, img2,img1, img1,img1, img1, img1, img1,img1, img1])\n",
        "\n",
        "resp = model.detect_faces_on_frames(frames=img)\n",
        "\n",
        "\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "c5Wz1jiUBxfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FDetector"
      ],
      "metadata": {
        "id": "NxOP0DUXB6ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import threading\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gdown\n",
        "import tensorflow as tf\n",
        "from retinaface.commons.logger import Logger\n",
        "\n",
        "from retinaface.model import retinaface_model\n",
        "from retinaface.commons import preprocess, postprocess\n",
        "from retinaface.commons.logger import Logger\n",
        "\n",
        "\n",
        "# configurations\n",
        "import numpy as np\n",
        "\n",
        "tf_version = int(tf.__version__.split(\".\", maxsplit=1)[0])\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "        Input,\n",
        "        BatchNormalization,\n",
        "        ZeroPadding2D,\n",
        "        Conv2D,\n",
        "        ReLU,\n",
        "        MaxPool2D,\n",
        "        Add,\n",
        "        UpSampling2D,\n",
        "        concatenate,\n",
        "        Softmax\n",
        ")\n",
        "\n",
        "class FDetector():\n",
        "    def __init__(self):\n",
        "        self.model = tf.function(\n",
        "            self.__build_model(),\n",
        "            input_signature=(tf.TensorSpec(shape=[None, None, None, 3], dtype=np.float32),)\n",
        "        )\n",
        "    def __build_model(self) -> Model:\n",
        "        \"\"\"\n",
        "        Build RetinaFace model\n",
        "        \"\"\"\n",
        "        data = Input(dtype=tf.float32, shape=(None, None, 3), name=\"data\")\n",
        "\n",
        "        bn_data = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn_data\", trainable=False)(\n",
        "            data\n",
        "        )\n",
        "\n",
        "        conv0_pad = ZeroPadding2D(padding=tuple([3, 3]))(bn_data)\n",
        "\n",
        "        conv0 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(7, 7),\n",
        "            name=\"conv0\",\n",
        "            strides=[2, 2],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(conv0_pad)\n",
        "\n",
        "        bn0 = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn0\", trainable=False)(conv0)\n",
        "\n",
        "        relu0 = ReLU(name=\"relu0\")(bn0)\n",
        "\n",
        "        pooling0_pad = ZeroPadding2D(padding=tuple([1, 1]))(relu0)\n",
        "\n",
        "        pooling0 = MaxPool2D((3, 3), (2, 2), padding=\"valid\", name=\"pooling0\")(pooling0_pad)\n",
        "\n",
        "        stage1_unit1_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn1\", trainable=False\n",
        "        )(pooling0)\n",
        "\n",
        "        stage1_unit1_relu1 = ReLU(name=\"stage1_unit1_relu1\")(stage1_unit1_bn1)\n",
        "\n",
        "        stage1_unit1_conv1 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage1_unit1_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit1_relu1)\n",
        "\n",
        "        stage1_unit1_sc = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage1_unit1_sc\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit1_relu1)\n",
        "\n",
        "        stage1_unit1_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn2\", trainable=False\n",
        "        )(stage1_unit1_conv1)\n",
        "\n",
        "        stage1_unit1_relu2 = ReLU(name=\"stage1_unit1_relu2\")(stage1_unit1_bn2)\n",
        "\n",
        "        stage1_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit1_relu2)\n",
        "\n",
        "        stage1_unit1_conv2 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage1_unit1_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit1_conv2_pad)\n",
        "\n",
        "        stage1_unit1_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit1_bn3\", trainable=False\n",
        "        )(stage1_unit1_conv2)\n",
        "\n",
        "        stage1_unit1_relu3 = ReLU(name=\"stage1_unit1_relu3\")(stage1_unit1_bn3)\n",
        "\n",
        "        stage1_unit1_conv3 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage1_unit1_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit1_relu3)\n",
        "\n",
        "        plus0_v1 = Add()([stage1_unit1_conv3, stage1_unit1_sc])\n",
        "\n",
        "        stage1_unit2_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn1\", trainable=False\n",
        "        )(plus0_v1)\n",
        "\n",
        "        stage1_unit2_relu1 = ReLU(name=\"stage1_unit2_relu1\")(stage1_unit2_bn1)\n",
        "\n",
        "        stage1_unit2_conv1 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage1_unit2_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit2_relu1)\n",
        "\n",
        "        stage1_unit2_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn2\", trainable=False\n",
        "        )(stage1_unit2_conv1)\n",
        "\n",
        "        stage1_unit2_relu2 = ReLU(name=\"stage1_unit2_relu2\")(stage1_unit2_bn2)\n",
        "\n",
        "        stage1_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit2_relu2)\n",
        "\n",
        "        stage1_unit2_conv2 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage1_unit2_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit2_conv2_pad)\n",
        "\n",
        "        stage1_unit2_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit2_bn3\", trainable=False\n",
        "        )(stage1_unit2_conv2)\n",
        "\n",
        "        stage1_unit2_relu3 = ReLU(name=\"stage1_unit2_relu3\")(stage1_unit2_bn3)\n",
        "\n",
        "        stage1_unit2_conv3 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage1_unit2_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit2_relu3)\n",
        "\n",
        "        plus1_v2 = Add()([stage1_unit2_conv3, plus0_v1])\n",
        "\n",
        "        stage1_unit3_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn1\", trainable=False\n",
        "        )(plus1_v2)\n",
        "\n",
        "        stage1_unit3_relu1 = ReLU(name=\"stage1_unit3_relu1\")(stage1_unit3_bn1)\n",
        "\n",
        "        stage1_unit3_conv1 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage1_unit3_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit3_relu1)\n",
        "\n",
        "        stage1_unit3_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn2\", trainable=False\n",
        "        )(stage1_unit3_conv1)\n",
        "\n",
        "        stage1_unit3_relu2 = ReLU(name=\"stage1_unit3_relu2\")(stage1_unit3_bn2)\n",
        "\n",
        "        stage1_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage1_unit3_relu2)\n",
        "\n",
        "        stage1_unit3_conv2 = Conv2D(\n",
        "            filters=64,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage1_unit3_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit3_conv2_pad)\n",
        "\n",
        "        stage1_unit3_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage1_unit3_bn3\", trainable=False\n",
        "        )(stage1_unit3_conv2)\n",
        "\n",
        "        stage1_unit3_relu3 = ReLU(name=\"stage1_unit3_relu3\")(stage1_unit3_bn3)\n",
        "\n",
        "        stage1_unit3_conv3 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage1_unit3_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage1_unit3_relu3)\n",
        "\n",
        "        plus2 = Add()([stage1_unit3_conv3, plus1_v2])\n",
        "\n",
        "        stage2_unit1_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn1\", trainable=False\n",
        "        )(plus2)\n",
        "\n",
        "        stage2_unit1_relu1 = ReLU(name=\"stage2_unit1_relu1\")(stage2_unit1_bn1)\n",
        "\n",
        "        stage2_unit1_conv1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit1_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit1_relu1)\n",
        "\n",
        "        stage2_unit1_sc = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit1_sc\",\n",
        "            strides=[2, 2],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit1_relu1)\n",
        "\n",
        "        stage2_unit1_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn2\", trainable=False\n",
        "        )(stage2_unit1_conv1)\n",
        "\n",
        "        stage2_unit1_relu2 = ReLU(name=\"stage2_unit1_relu2\")(stage2_unit1_bn2)\n",
        "\n",
        "        stage2_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit1_relu2)\n",
        "\n",
        "        stage2_unit1_conv2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage2_unit1_conv2\",\n",
        "            strides=[2, 2],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit1_conv2_pad)\n",
        "\n",
        "        stage2_unit1_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit1_bn3\", trainable=False\n",
        "        )(stage2_unit1_conv2)\n",
        "\n",
        "        stage2_unit1_relu3 = ReLU(name=\"stage2_unit1_relu3\")(stage2_unit1_bn3)\n",
        "\n",
        "        stage2_unit1_conv3 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit1_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit1_relu3)\n",
        "\n",
        "        plus3 = Add()([stage2_unit1_conv3, stage2_unit1_sc])\n",
        "\n",
        "        stage2_unit2_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn1\", trainable=False\n",
        "        )(plus3)\n",
        "\n",
        "        stage2_unit2_relu1 = ReLU(name=\"stage2_unit2_relu1\")(stage2_unit2_bn1)\n",
        "\n",
        "        stage2_unit2_conv1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit2_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit2_relu1)\n",
        "\n",
        "        stage2_unit2_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn2\", trainable=False\n",
        "        )(stage2_unit2_conv1)\n",
        "\n",
        "        stage2_unit2_relu2 = ReLU(name=\"stage2_unit2_relu2\")(stage2_unit2_bn2)\n",
        "\n",
        "        stage2_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit2_relu2)\n",
        "\n",
        "        stage2_unit2_conv2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage2_unit2_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit2_conv2_pad)\n",
        "\n",
        "        stage2_unit2_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit2_bn3\", trainable=False\n",
        "        )(stage2_unit2_conv2)\n",
        "\n",
        "        stage2_unit2_relu3 = ReLU(name=\"stage2_unit2_relu3\")(stage2_unit2_bn3)\n",
        "\n",
        "        stage2_unit2_conv3 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit2_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit2_relu3)\n",
        "\n",
        "        plus4 = Add()([stage2_unit2_conv3, plus3])\n",
        "\n",
        "        stage2_unit3_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn1\", trainable=False\n",
        "        )(plus4)\n",
        "\n",
        "        stage2_unit3_relu1 = ReLU(name=\"stage2_unit3_relu1\")(stage2_unit3_bn1)\n",
        "\n",
        "        stage2_unit3_conv1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit3_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit3_relu1)\n",
        "\n",
        "        stage2_unit3_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn2\", trainable=False\n",
        "        )(stage2_unit3_conv1)\n",
        "\n",
        "        stage2_unit3_relu2 = ReLU(name=\"stage2_unit3_relu2\")(stage2_unit3_bn2)\n",
        "\n",
        "        stage2_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit3_relu2)\n",
        "\n",
        "        stage2_unit3_conv2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage2_unit3_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit3_conv2_pad)\n",
        "\n",
        "        stage2_unit3_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit3_bn3\", trainable=False\n",
        "        )(stage2_unit3_conv2)\n",
        "\n",
        "        stage2_unit3_relu3 = ReLU(name=\"stage2_unit3_relu3\")(stage2_unit3_bn3)\n",
        "\n",
        "        stage2_unit3_conv3 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit3_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit3_relu3)\n",
        "\n",
        "        plus5 = Add()([stage2_unit3_conv3, plus4])\n",
        "\n",
        "        stage2_unit4_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn1\", trainable=False\n",
        "        )(plus5)\n",
        "\n",
        "        stage2_unit4_relu1 = ReLU(name=\"stage2_unit4_relu1\")(stage2_unit4_bn1)\n",
        "\n",
        "        stage2_unit4_conv1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit4_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit4_relu1)\n",
        "\n",
        "        stage2_unit4_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn2\", trainable=False\n",
        "        )(stage2_unit4_conv1)\n",
        "\n",
        "        stage2_unit4_relu2 = ReLU(name=\"stage2_unit4_relu2\")(stage2_unit4_bn2)\n",
        "\n",
        "        stage2_unit4_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage2_unit4_relu2)\n",
        "\n",
        "        stage2_unit4_conv2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage2_unit4_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit4_conv2_pad)\n",
        "\n",
        "        stage2_unit4_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage2_unit4_bn3\", trainable=False\n",
        "        )(stage2_unit4_conv2)\n",
        "\n",
        "        stage2_unit4_relu3 = ReLU(name=\"stage2_unit4_relu3\")(stage2_unit4_bn3)\n",
        "\n",
        "        stage2_unit4_conv3 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage2_unit4_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage2_unit4_relu3)\n",
        "\n",
        "        plus6 = Add()([stage2_unit4_conv3, plus5])\n",
        "\n",
        "        stage3_unit1_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn1\", trainable=False\n",
        "        )(plus6)\n",
        "\n",
        "        stage3_unit1_relu1 = ReLU(name=\"stage3_unit1_relu1\")(stage3_unit1_bn1)\n",
        "\n",
        "        stage3_unit1_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit1_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit1_relu1)\n",
        "\n",
        "        stage3_unit1_sc = Conv2D(\n",
        "            filters=1024,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit1_sc\",\n",
        "            strides=[2, 2],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit1_relu1)\n",
        "\n",
        "        stage3_unit1_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn2\", trainable=False\n",
        "        )(stage3_unit1_conv1)\n",
        "\n",
        "        stage3_unit1_relu2 = ReLU(name=\"stage3_unit1_relu2\")(stage3_unit1_bn2)\n",
        "\n",
        "        stage3_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit1_relu2)\n",
        "\n",
        "        stage3_unit1_conv2 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage3_unit1_conv2\",\n",
        "            strides=[2, 2],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit1_conv2_pad)\n",
        "\n",
        "        ssh_m1_red_conv = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"ssh_m1_red_conv\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(stage3_unit1_relu2)\n",
        "\n",
        "        stage3_unit1_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit1_bn3\", trainable=False\n",
        "        )(stage3_unit1_conv2)\n",
        "\n",
        "        ssh_m1_red_conv_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m1_red_conv_bn\", trainable=False\n",
        "        )(ssh_m1_red_conv)\n",
        "\n",
        "        stage3_unit1_relu3 = ReLU(name=\"stage3_unit1_relu3\")(stage3_unit1_bn3)\n",
        "\n",
        "        ssh_m1_red_conv_relu = ReLU(name=\"ssh_m1_red_conv_relu\")(ssh_m1_red_conv_bn)\n",
        "\n",
        "        stage3_unit1_conv3 = Conv2D(\n",
        "            filters=1024,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit1_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit1_relu3)\n",
        "\n",
        "        plus7 = Add()([stage3_unit1_conv3, stage3_unit1_sc])\n",
        "\n",
        "        stage3_unit2_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn1\", trainable=False\n",
        "        )(plus7)\n",
        "\n",
        "        stage3_unit2_relu1 = ReLU(name=\"stage3_unit2_relu1\")(stage3_unit2_bn1)\n",
        "\n",
        "        stage3_unit2_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit2_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit2_relu1)\n",
        "\n",
        "        stage3_unit2_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn2\", trainable=False\n",
        "        )(stage3_unit2_conv1)\n",
        "\n",
        "        stage3_unit2_relu2 = ReLU(name=\"stage3_unit2_relu2\")(stage3_unit2_bn2)\n",
        "\n",
        "        stage3_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit2_relu2)\n",
        "\n",
        "        stage3_unit2_conv2 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage3_unit2_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit2_conv2_pad)\n",
        "\n",
        "        stage3_unit2_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit2_bn3\", trainable=False\n",
        "        )(stage3_unit2_conv2)\n",
        "\n",
        "        stage3_unit2_relu3 = ReLU(name=\"stage3_unit2_relu3\")(stage3_unit2_bn3)\n",
        "\n",
        "        stage3_unit2_conv3 = Conv2D(\n",
        "            filters=1024,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit2_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit2_relu3)\n",
        "\n",
        "        plus8 = Add()([stage3_unit2_conv3, plus7])\n",
        "\n",
        "        stage3_unit3_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn1\", trainable=False\n",
        "        )(plus8)\n",
        "\n",
        "        stage3_unit3_relu1 = ReLU(name=\"stage3_unit3_relu1\")(stage3_unit3_bn1)\n",
        "\n",
        "        stage3_unit3_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit3_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit3_relu1)\n",
        "\n",
        "        stage3_unit3_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn2\", trainable=False\n",
        "        )(stage3_unit3_conv1)\n",
        "\n",
        "        stage3_unit3_relu2 = ReLU(name=\"stage3_unit3_relu2\")(stage3_unit3_bn2)\n",
        "\n",
        "        stage3_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit3_relu2)\n",
        "\n",
        "        stage3_unit3_conv2 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage3_unit3_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit3_conv2_pad)\n",
        "\n",
        "        stage3_unit3_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit3_bn3\", trainable=False\n",
        "        )(stage3_unit3_conv2)\n",
        "\n",
        "        stage3_unit3_relu3 = ReLU(name=\"stage3_unit3_relu3\")(stage3_unit3_bn3)\n",
        "\n",
        "        stage3_unit3_conv3 = Conv2D(\n",
        "            filters=1024,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit3_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit3_relu3)\n",
        "\n",
        "        plus9 = Add()([stage3_unit3_conv3, plus8])\n",
        "\n",
        "        stage3_unit4_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn1\", trainable=False\n",
        "        )(plus9)\n",
        "\n",
        "        stage3_unit4_relu1 = ReLU(name=\"stage3_unit4_relu1\")(stage3_unit4_bn1)\n",
        "\n",
        "        stage3_unit4_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit4_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit4_relu1)\n",
        "\n",
        "        stage3_unit4_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn2\", trainable=False\n",
        "        )(stage3_unit4_conv1)\n",
        "\n",
        "        stage3_unit4_relu2 = ReLU(name=\"stage3_unit4_relu2\")(stage3_unit4_bn2)\n",
        "\n",
        "        stage3_unit4_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit4_relu2)\n",
        "\n",
        "        stage3_unit4_conv2 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage3_unit4_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit4_conv2_pad)\n",
        "\n",
        "        stage3_unit4_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit4_bn3\", trainable=False\n",
        "        )(stage3_unit4_conv2)\n",
        "\n",
        "        stage3_unit4_relu3 = ReLU(name=\"stage3_unit4_relu3\")(stage3_unit4_bn3)\n",
        "\n",
        "        stage3_unit4_conv3 = Conv2D(\n",
        "            filters=1024,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit4_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit4_relu3)\n",
        "\n",
        "        plus10 = Add()([stage3_unit4_conv3, plus9])\n",
        "\n",
        "        stage3_unit5_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn1\", trainable=False\n",
        "        )(plus10)\n",
        "\n",
        "        stage3_unit5_relu1 = ReLU(name=\"stage3_unit5_relu1\")(stage3_unit5_bn1)\n",
        "\n",
        "        stage3_unit5_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit5_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit5_relu1)\n",
        "\n",
        "        stage3_unit5_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn2\", trainable=False\n",
        "        )(stage3_unit5_conv1)\n",
        "\n",
        "        stage3_unit5_relu2 = ReLU(name=\"stage3_unit5_relu2\")(stage3_unit5_bn2)\n",
        "\n",
        "        stage3_unit5_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit5_relu2)\n",
        "\n",
        "        stage3_unit5_conv2 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage3_unit5_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit5_conv2_pad)\n",
        "\n",
        "        stage3_unit5_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit5_bn3\", trainable=False\n",
        "        )(stage3_unit5_conv2)\n",
        "\n",
        "        stage3_unit5_relu3 = ReLU(name=\"stage3_unit5_relu3\")(stage3_unit5_bn3)\n",
        "\n",
        "        stage3_unit5_conv3 = Conv2D(\n",
        "            filters=1024,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit5_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit5_relu3)\n",
        "\n",
        "        plus11 = Add()([stage3_unit5_conv3, plus10])\n",
        "\n",
        "        stage3_unit6_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn1\", trainable=False\n",
        "        )(plus11)\n",
        "\n",
        "        stage3_unit6_relu1 = ReLU(name=\"stage3_unit6_relu1\")(stage3_unit6_bn1)\n",
        "\n",
        "        stage3_unit6_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit6_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit6_relu1)\n",
        "\n",
        "        stage3_unit6_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn2\", trainable=False\n",
        "        )(stage3_unit6_conv1)\n",
        "\n",
        "        stage3_unit6_relu2 = ReLU(name=\"stage3_unit6_relu2\")(stage3_unit6_bn2)\n",
        "\n",
        "        stage3_unit6_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage3_unit6_relu2)\n",
        "\n",
        "        stage3_unit6_conv2 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage3_unit6_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit6_conv2_pad)\n",
        "\n",
        "        stage3_unit6_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage3_unit6_bn3\", trainable=False\n",
        "        )(stage3_unit6_conv2)\n",
        "\n",
        "        stage3_unit6_relu3 = ReLU(name=\"stage3_unit6_relu3\")(stage3_unit6_bn3)\n",
        "\n",
        "        stage3_unit6_conv3 = Conv2D(\n",
        "            filters=1024,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage3_unit6_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage3_unit6_relu3)\n",
        "\n",
        "        plus12 = Add()([stage3_unit6_conv3, plus11])\n",
        "\n",
        "        stage4_unit1_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn1\", trainable=False\n",
        "        )(plus12)\n",
        "\n",
        "        stage4_unit1_relu1 = ReLU(name=\"stage4_unit1_relu1\")(stage4_unit1_bn1)\n",
        "\n",
        "        stage4_unit1_conv1 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage4_unit1_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit1_relu1)\n",
        "\n",
        "        stage4_unit1_sc = Conv2D(\n",
        "            filters=2048,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage4_unit1_sc\",\n",
        "            strides=[2, 2],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit1_relu1)\n",
        "\n",
        "        stage4_unit1_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn2\", trainable=False\n",
        "        )(stage4_unit1_conv1)\n",
        "\n",
        "        stage4_unit1_relu2 = ReLU(name=\"stage4_unit1_relu2\")(stage4_unit1_bn2)\n",
        "\n",
        "        stage4_unit1_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit1_relu2)\n",
        "\n",
        "        stage4_unit1_conv2 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage4_unit1_conv2\",\n",
        "            strides=[2, 2],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit1_conv2_pad)\n",
        "\n",
        "        ssh_c2_lateral = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"ssh_c2_lateral\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(stage4_unit1_relu2)\n",
        "\n",
        "        stage4_unit1_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit1_bn3\", trainable=False\n",
        "        )(stage4_unit1_conv2)\n",
        "\n",
        "        ssh_c2_lateral_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_c2_lateral_bn\", trainable=False\n",
        "        )(ssh_c2_lateral)\n",
        "\n",
        "        stage4_unit1_relu3 = ReLU(name=\"stage4_unit1_relu3\")(stage4_unit1_bn3)\n",
        "\n",
        "        ssh_c2_lateral_relu = ReLU(name=\"ssh_c2_lateral_relu\")(ssh_c2_lateral_bn)\n",
        "\n",
        "        stage4_unit1_conv3 = Conv2D(\n",
        "            filters=2048,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage4_unit1_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit1_relu3)\n",
        "\n",
        "        plus13 = Add()([stage4_unit1_conv3, stage4_unit1_sc])\n",
        "\n",
        "        stage4_unit2_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn1\", trainable=False\n",
        "        )(plus13)\n",
        "\n",
        "        stage4_unit2_relu1 = ReLU(name=\"stage4_unit2_relu1\")(stage4_unit2_bn1)\n",
        "\n",
        "        stage4_unit2_conv1 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage4_unit2_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit2_relu1)\n",
        "\n",
        "        stage4_unit2_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn2\", trainable=False\n",
        "        )(stage4_unit2_conv1)\n",
        "\n",
        "        stage4_unit2_relu2 = ReLU(name=\"stage4_unit2_relu2\")(stage4_unit2_bn2)\n",
        "\n",
        "        stage4_unit2_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit2_relu2)\n",
        "\n",
        "        stage4_unit2_conv2 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage4_unit2_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit2_conv2_pad)\n",
        "\n",
        "        stage4_unit2_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit2_bn3\", trainable=False\n",
        "        )(stage4_unit2_conv2)\n",
        "\n",
        "        stage4_unit2_relu3 = ReLU(name=\"stage4_unit2_relu3\")(stage4_unit2_bn3)\n",
        "\n",
        "        stage4_unit2_conv3 = Conv2D(\n",
        "            filters=2048,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage4_unit2_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit2_relu3)\n",
        "\n",
        "        plus14 = Add()([stage4_unit2_conv3, plus13])\n",
        "\n",
        "        stage4_unit3_bn1 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn1\", trainable=False\n",
        "        )(plus14)\n",
        "\n",
        "        stage4_unit3_relu1 = ReLU(name=\"stage4_unit3_relu1\")(stage4_unit3_bn1)\n",
        "\n",
        "        stage4_unit3_conv1 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage4_unit3_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit3_relu1)\n",
        "\n",
        "        stage4_unit3_bn2 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn2\", trainable=False\n",
        "        )(stage4_unit3_conv1)\n",
        "\n",
        "        stage4_unit3_relu2 = ReLU(name=\"stage4_unit3_relu2\")(stage4_unit3_bn2)\n",
        "\n",
        "        stage4_unit3_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(stage4_unit3_relu2)\n",
        "\n",
        "        stage4_unit3_conv2 = Conv2D(\n",
        "            filters=512,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"stage4_unit3_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit3_conv2_pad)\n",
        "\n",
        "        stage4_unit3_bn3 = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"stage4_unit3_bn3\", trainable=False\n",
        "        )(stage4_unit3_conv2)\n",
        "\n",
        "        stage4_unit3_relu3 = ReLU(name=\"stage4_unit3_relu3\")(stage4_unit3_bn3)\n",
        "\n",
        "        stage4_unit3_conv3 = Conv2D(\n",
        "            filters=2048,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"stage4_unit3_conv3\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=False,\n",
        "        )(stage4_unit3_relu3)\n",
        "\n",
        "        plus15 = Add()([stage4_unit3_conv3, plus14])\n",
        "\n",
        "        bn1 = BatchNormalization(epsilon=1.9999999494757503e-05, name=\"bn1\", trainable=False)(plus15)\n",
        "\n",
        "        relu1 = ReLU(name=\"relu1\")(bn1)\n",
        "\n",
        "        ssh_c3_lateral = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"ssh_c3_lateral\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(relu1)\n",
        "\n",
        "        ssh_c3_lateral_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_c3_lateral_bn\", trainable=False\n",
        "        )(ssh_c3_lateral)\n",
        "\n",
        "        ssh_c3_lateral_relu = ReLU(name=\"ssh_c3_lateral_relu\")(ssh_c3_lateral_bn)\n",
        "\n",
        "        ssh_m3_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c3_lateral_relu)\n",
        "\n",
        "        ssh_m3_det_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m3_det_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_conv1_pad)\n",
        "\n",
        "        ssh_m3_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c3_lateral_relu)\n",
        "\n",
        "        ssh_m3_det_context_conv1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m3_det_context_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_context_conv1_pad)\n",
        "\n",
        "        ssh_c3_up = UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=\"ssh_c3_up\")(\n",
        "            ssh_c3_lateral_relu\n",
        "        )\n",
        "\n",
        "        ssh_m3_det_conv1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_conv1_bn\", trainable=False\n",
        "        )(ssh_m3_det_conv1)\n",
        "\n",
        "        ssh_m3_det_context_conv1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv1_bn\", trainable=False\n",
        "        )(ssh_m3_det_context_conv1)\n",
        "\n",
        "        x1_shape = tf.shape(ssh_c3_up)\n",
        "        x2_shape = tf.shape(ssh_c2_lateral_relu)\n",
        "        offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
        "        size = [-1, x2_shape[1], x2_shape[2], -1]\n",
        "        crop0 = tf.slice(ssh_c3_up, offsets, size, \"crop0\")\n",
        "\n",
        "        ssh_m3_det_context_conv1_relu = ReLU(name=\"ssh_m3_det_context_conv1_relu\")(\n",
        "            ssh_m3_det_context_conv1_bn\n",
        "        )\n",
        "\n",
        "        plus0_v2 = Add()([ssh_c2_lateral_relu, crop0])\n",
        "\n",
        "        ssh_m3_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m3_det_context_conv1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m3_det_context_conv2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m3_det_context_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_context_conv2_pad)\n",
        "\n",
        "        ssh_m3_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m3_det_context_conv1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m3_det_context_conv3_1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m3_det_context_conv3_1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_context_conv3_1_pad)\n",
        "\n",
        "        ssh_c2_aggr_pad = ZeroPadding2D(padding=tuple([1, 1]))(plus0_v2)\n",
        "\n",
        "        ssh_c2_aggr = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_c2_aggr\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_c2_aggr_pad)\n",
        "\n",
        "        ssh_m3_det_context_conv2_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv2_bn\", trainable=False\n",
        "        )(ssh_m3_det_context_conv2)\n",
        "\n",
        "        ssh_m3_det_context_conv3_1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv3_1_bn\", trainable=False\n",
        "        )(ssh_m3_det_context_conv3_1)\n",
        "\n",
        "        ssh_c2_aggr_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_c2_aggr_bn\", trainable=False\n",
        "        )(ssh_c2_aggr)\n",
        "\n",
        "        ssh_m3_det_context_conv3_1_relu = ReLU(name=\"ssh_m3_det_context_conv3_1_relu\")(\n",
        "            ssh_m3_det_context_conv3_1_bn\n",
        "        )\n",
        "\n",
        "        ssh_c2_aggr_relu = ReLU(name=\"ssh_c2_aggr_relu\")(ssh_c2_aggr_bn)\n",
        "\n",
        "        ssh_m3_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m3_det_context_conv3_1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m3_det_context_conv3_2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m3_det_context_conv3_2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_context_conv3_2_pad)\n",
        "\n",
        "        ssh_m2_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c2_aggr_relu)\n",
        "\n",
        "        ssh_m2_det_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m2_det_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_conv1_pad)\n",
        "\n",
        "        ssh_m2_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c2_aggr_relu)\n",
        "\n",
        "        ssh_m2_det_context_conv1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m2_det_context_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_context_conv1_pad)\n",
        "\n",
        "        ssh_m2_red_up = UpSampling2D(size=(2, 2), interpolation=\"nearest\", name=\"ssh_m2_red_up\")(\n",
        "            ssh_c2_aggr_relu\n",
        "        )\n",
        "\n",
        "        ssh_m3_det_context_conv3_2_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m3_det_context_conv3_2_bn\", trainable=False\n",
        "        )(ssh_m3_det_context_conv3_2)\n",
        "\n",
        "        ssh_m2_det_conv1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_conv1_bn\", trainable=False\n",
        "        )(ssh_m2_det_conv1)\n",
        "\n",
        "        ssh_m2_det_context_conv1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv1_bn\", trainable=False\n",
        "        )(ssh_m2_det_context_conv1)\n",
        "\n",
        "        x1_shape = tf.shape(ssh_m2_red_up)\n",
        "        x2_shape = tf.shape(ssh_m1_red_conv_relu)\n",
        "        offsets = [0, (x1_shape[1] - x2_shape[1]) // 2, (x1_shape[2] - x2_shape[2]) // 2, 0]\n",
        "        size = [-1, x2_shape[1], x2_shape[2], -1]\n",
        "        crop1 = tf.slice(ssh_m2_red_up, offsets, size, \"crop1\")\n",
        "\n",
        "        ssh_m3_det_concat = concatenate(\n",
        "            [ssh_m3_det_conv1_bn, ssh_m3_det_context_conv2_bn, ssh_m3_det_context_conv3_2_bn],\n",
        "            3,\n",
        "            name=\"ssh_m3_det_concat\",\n",
        "        )\n",
        "\n",
        "        ssh_m2_det_context_conv1_relu = ReLU(name=\"ssh_m2_det_context_conv1_relu\")(\n",
        "            ssh_m2_det_context_conv1_bn\n",
        "        )\n",
        "\n",
        "        plus1_v1 = Add()([ssh_m1_red_conv_relu, crop1])\n",
        "\n",
        "        ssh_m3_det_concat_relu = ReLU(name=\"ssh_m3_det_concat_relu\")(ssh_m3_det_concat)\n",
        "\n",
        "        ssh_m2_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m2_det_context_conv1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m2_det_context_conv2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m2_det_context_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_context_conv2_pad)\n",
        "\n",
        "        ssh_m2_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m2_det_context_conv1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m2_det_context_conv3_1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m2_det_context_conv3_1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_context_conv3_1_pad)\n",
        "\n",
        "        ssh_c1_aggr_pad = ZeroPadding2D(padding=tuple([1, 1]))(plus1_v1)\n",
        "\n",
        "        ssh_c1_aggr = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_c1_aggr\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_c1_aggr_pad)\n",
        "\n",
        "        face_rpn_cls_score_stride32 = Conv2D(\n",
        "            filters=4,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_cls_score_stride32\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_concat_relu)\n",
        "\n",
        "        inter_1 = concatenate(\n",
        "            [face_rpn_cls_score_stride32[:, :, :, 0], face_rpn_cls_score_stride32[:, :, :, 1]], axis=1\n",
        "        )\n",
        "        inter_2 = concatenate(\n",
        "            [face_rpn_cls_score_stride32[:, :, :, 2], face_rpn_cls_score_stride32[:, :, :, 3]], axis=1\n",
        "        )\n",
        "        final = tf.stack([inter_1, inter_2])\n",
        "        face_rpn_cls_score_reshape_stride32 = tf.transpose(\n",
        "            final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride32\"\n",
        "        )\n",
        "\n",
        "        face_rpn_bbox_pred_stride32 = Conv2D(\n",
        "            filters=8,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_bbox_pred_stride32\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_concat_relu)\n",
        "\n",
        "        face_rpn_landmark_pred_stride32 = Conv2D(\n",
        "            filters=20,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_landmark_pred_stride32\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m3_det_concat_relu)\n",
        "\n",
        "        ssh_m2_det_context_conv2_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv2_bn\", trainable=False\n",
        "        )(ssh_m2_det_context_conv2)\n",
        "\n",
        "        ssh_m2_det_context_conv3_1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv3_1_bn\", trainable=False\n",
        "        )(ssh_m2_det_context_conv3_1)\n",
        "\n",
        "        ssh_c1_aggr_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_c1_aggr_bn\", trainable=False\n",
        "        )(ssh_c1_aggr)\n",
        "\n",
        "        ssh_m2_det_context_conv3_1_relu = ReLU(name=\"ssh_m2_det_context_conv3_1_relu\")(\n",
        "            ssh_m2_det_context_conv3_1_bn\n",
        "        )\n",
        "\n",
        "        ssh_c1_aggr_relu = ReLU(name=\"ssh_c1_aggr_relu\")(ssh_c1_aggr_bn)\n",
        "\n",
        "        face_rpn_cls_prob_stride32 = Softmax(name=\"face_rpn_cls_prob_stride32\")(\n",
        "            face_rpn_cls_score_reshape_stride32\n",
        "        )\n",
        "\n",
        "        input_shape = [tf.shape(face_rpn_cls_prob_stride32)[k] for k in range(4)]\n",
        "        sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
        "        inter_1 = face_rpn_cls_prob_stride32[:, 0:sz, :, 0]\n",
        "        inter_2 = face_rpn_cls_prob_stride32[:, 0:sz, :, 1]\n",
        "        inter_3 = face_rpn_cls_prob_stride32[:, sz:, :, 0]\n",
        "        inter_4 = face_rpn_cls_prob_stride32[:, sz:, :, 1]\n",
        "        final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
        "        face_rpn_cls_prob_reshape_stride32 = tf.transpose(\n",
        "            final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride32\"\n",
        "        )\n",
        "\n",
        "        ssh_m2_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m2_det_context_conv3_1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m2_det_context_conv3_2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m2_det_context_conv3_2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_context_conv3_2_pad)\n",
        "\n",
        "        ssh_m1_det_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c1_aggr_relu)\n",
        "\n",
        "        ssh_m1_det_conv1 = Conv2D(\n",
        "            filters=256,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m1_det_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_conv1_pad)\n",
        "\n",
        "        ssh_m1_det_context_conv1_pad = ZeroPadding2D(padding=tuple([1, 1]))(ssh_c1_aggr_relu)\n",
        "\n",
        "        ssh_m1_det_context_conv1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m1_det_context_conv1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_context_conv1_pad)\n",
        "\n",
        "        ssh_m2_det_context_conv3_2_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m2_det_context_conv3_2_bn\", trainable=False\n",
        "        )(ssh_m2_det_context_conv3_2)\n",
        "\n",
        "        ssh_m1_det_conv1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_conv1_bn\", trainable=False\n",
        "        )(ssh_m1_det_conv1)\n",
        "\n",
        "        ssh_m1_det_context_conv1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv1_bn\", trainable=False\n",
        "        )(ssh_m1_det_context_conv1)\n",
        "\n",
        "        ssh_m2_det_concat = concatenate(\n",
        "            [ssh_m2_det_conv1_bn, ssh_m2_det_context_conv2_bn, ssh_m2_det_context_conv3_2_bn],\n",
        "            3,\n",
        "            name=\"ssh_m2_det_concat\",\n",
        "        )\n",
        "\n",
        "        ssh_m1_det_context_conv1_relu = ReLU(name=\"ssh_m1_det_context_conv1_relu\")(\n",
        "            ssh_m1_det_context_conv1_bn\n",
        "        )\n",
        "\n",
        "        ssh_m2_det_concat_relu = ReLU(name=\"ssh_m2_det_concat_relu\")(ssh_m2_det_concat)\n",
        "\n",
        "        ssh_m1_det_context_conv2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m1_det_context_conv1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m1_det_context_conv2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m1_det_context_conv2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_context_conv2_pad)\n",
        "\n",
        "        ssh_m1_det_context_conv3_1_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m1_det_context_conv1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m1_det_context_conv3_1 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m1_det_context_conv3_1\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_context_conv3_1_pad)\n",
        "\n",
        "        face_rpn_cls_score_stride16 = Conv2D(\n",
        "            filters=4,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_cls_score_stride16\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_concat_relu)\n",
        "\n",
        "        inter_1 = concatenate(\n",
        "            [face_rpn_cls_score_stride16[:, :, :, 0], face_rpn_cls_score_stride16[:, :, :, 1]], axis=1\n",
        "        )\n",
        "        inter_2 = concatenate(\n",
        "            [face_rpn_cls_score_stride16[:, :, :, 2], face_rpn_cls_score_stride16[:, :, :, 3]], axis=1\n",
        "        )\n",
        "        final = tf.stack([inter_1, inter_2])\n",
        "        face_rpn_cls_score_reshape_stride16 = tf.transpose(\n",
        "            final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride16\"\n",
        "        )\n",
        "\n",
        "        face_rpn_bbox_pred_stride16 = Conv2D(\n",
        "            filters=8,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_bbox_pred_stride16\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_concat_relu)\n",
        "\n",
        "        face_rpn_landmark_pred_stride16 = Conv2D(\n",
        "            filters=20,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_landmark_pred_stride16\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m2_det_concat_relu)\n",
        "\n",
        "        ssh_m1_det_context_conv2_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv2_bn\", trainable=False\n",
        "        )(ssh_m1_det_context_conv2)\n",
        "\n",
        "        ssh_m1_det_context_conv3_1_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv3_1_bn\", trainable=False\n",
        "        )(ssh_m1_det_context_conv3_1)\n",
        "\n",
        "        ssh_m1_det_context_conv3_1_relu = ReLU(name=\"ssh_m1_det_context_conv3_1_relu\")(\n",
        "            ssh_m1_det_context_conv3_1_bn\n",
        "        )\n",
        "\n",
        "        face_rpn_cls_prob_stride16 = Softmax(name=\"face_rpn_cls_prob_stride16\")(\n",
        "            face_rpn_cls_score_reshape_stride16\n",
        "        )\n",
        "\n",
        "        input_shape = [tf.shape(face_rpn_cls_prob_stride16)[k] for k in range(4)]\n",
        "        sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
        "        inter_1 = face_rpn_cls_prob_stride16[:, 0:sz, :, 0]\n",
        "        inter_2 = face_rpn_cls_prob_stride16[:, 0:sz, :, 1]\n",
        "        inter_3 = face_rpn_cls_prob_stride16[:, sz:, :, 0]\n",
        "        inter_4 = face_rpn_cls_prob_stride16[:, sz:, :, 1]\n",
        "        final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
        "        face_rpn_cls_prob_reshape_stride16 = tf.transpose(\n",
        "            final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride16\"\n",
        "        )\n",
        "\n",
        "        ssh_m1_det_context_conv3_2_pad = ZeroPadding2D(padding=tuple([1, 1]))(\n",
        "            ssh_m1_det_context_conv3_1_relu\n",
        "        )\n",
        "\n",
        "        ssh_m1_det_context_conv3_2 = Conv2D(\n",
        "            filters=128,\n",
        "            kernel_size=(3, 3),\n",
        "            name=\"ssh_m1_det_context_conv3_2\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_context_conv3_2_pad)\n",
        "\n",
        "        ssh_m1_det_context_conv3_2_bn = BatchNormalization(\n",
        "            epsilon=1.9999999494757503e-05, name=\"ssh_m1_det_context_conv3_2_bn\", trainable=False\n",
        "        )(ssh_m1_det_context_conv3_2)\n",
        "\n",
        "        ssh_m1_det_concat = concatenate(\n",
        "            [ssh_m1_det_conv1_bn, ssh_m1_det_context_conv2_bn, ssh_m1_det_context_conv3_2_bn],\n",
        "            3,\n",
        "            name=\"ssh_m1_det_concat\",\n",
        "        )\n",
        "\n",
        "        ssh_m1_det_concat_relu = ReLU(name=\"ssh_m1_det_concat_relu\")(ssh_m1_det_concat)\n",
        "        face_rpn_cls_score_stride8 = Conv2D(\n",
        "            filters=4,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_cls_score_stride8\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_concat_relu)\n",
        "\n",
        "        inter_1 = concatenate(\n",
        "            [face_rpn_cls_score_stride8[:, :, :, 0], face_rpn_cls_score_stride8[:, :, :, 1]], axis=1\n",
        "        )\n",
        "        inter_2 = concatenate(\n",
        "            [face_rpn_cls_score_stride8[:, :, :, 2], face_rpn_cls_score_stride8[:, :, :, 3]], axis=1\n",
        "        )\n",
        "        final = tf.stack([inter_1, inter_2])\n",
        "        face_rpn_cls_score_reshape_stride8 = tf.transpose(\n",
        "            final, (1, 2, 3, 0), name=\"face_rpn_cls_score_reshape_stride8\"\n",
        "        )\n",
        "\n",
        "        face_rpn_bbox_pred_stride8 = Conv2D(\n",
        "            filters=8,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_bbox_pred_stride8\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_concat_relu)\n",
        "\n",
        "        face_rpn_landmark_pred_stride8 = Conv2D(\n",
        "            filters=20,\n",
        "            kernel_size=(1, 1),\n",
        "            name=\"face_rpn_landmark_pred_stride8\",\n",
        "            strides=[1, 1],\n",
        "            padding=\"VALID\",\n",
        "            use_bias=True,\n",
        "        )(ssh_m1_det_concat_relu)\n",
        "\n",
        "        face_rpn_cls_prob_stride8 = Softmax(name=\"face_rpn_cls_prob_stride8\")(\n",
        "            face_rpn_cls_score_reshape_stride8\n",
        "        )\n",
        "\n",
        "        input_shape = [tf.shape(face_rpn_cls_prob_stride8)[k] for k in range(4)]\n",
        "        sz = tf.dtypes.cast(input_shape[1] / 2, dtype=tf.int32)\n",
        "        inter_1 = face_rpn_cls_prob_stride8[:, 0:sz, :, 0]\n",
        "        inter_2 = face_rpn_cls_prob_stride8[:, 0:sz, :, 1]\n",
        "        inter_3 = face_rpn_cls_prob_stride8[:, sz:, :, 0]\n",
        "        inter_4 = face_rpn_cls_prob_stride8[:, sz:, :, 1]\n",
        "        final = tf.stack([inter_1, inter_3, inter_2, inter_4])\n",
        "        face_rpn_cls_prob_reshape_stride8 = tf.transpose(\n",
        "            final, (1, 2, 3, 0), name=\"face_rpn_cls_prob_reshape_stride8\"\n",
        "        )\n",
        "\n",
        "        model = Model(\n",
        "            inputs=data,\n",
        "            outputs=[\n",
        "                face_rpn_cls_prob_reshape_stride32,\n",
        "                face_rpn_bbox_pred_stride32,\n",
        "                face_rpn_landmark_pred_stride32,\n",
        "                face_rpn_cls_prob_reshape_stride16,\n",
        "                face_rpn_bbox_pred_stride16,\n",
        "                face_rpn_landmark_pred_stride16,\n",
        "                face_rpn_cls_prob_reshape_stride8,\n",
        "                face_rpn_bbox_pred_stride8,\n",
        "                face_rpn_landmark_pred_stride8,\n",
        "            ],\n",
        "        )\n",
        "        model.load_weights(\"d://models/retinaface.h5\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    def __determize_faces(self,img, net_out):\n",
        "        start = time.time()\n",
        "        threshold = 0.9\n",
        "        allow_upscaling = True\n",
        "        resp = {}\n",
        "       # ---------------------------\n",
        "        nms_threshold = 0.4\n",
        "        decay4 = 0.5\n",
        "        _feat_stride_fpn = [32, 16, 8]\n",
        "        _anchors_fpn = {\n",
        "            \"stride32\": np.array(\n",
        "                [[-248.0, -248.0, 263.0, 263.0],[-120.0, -120.0, 135.0, 135.0]], dtype=np.float32 ),\n",
        "            \"stride16\": np.array(\n",
        "                [[-56.0, -56.0, 71.0, 71.0], [-24.0, -24.0, 39.0, 39.0]], dtype=np.float32 ),\n",
        "            \"stride8\": np.array(\n",
        "                [[-8.0, -8.0, 23.0, 23.0], [0.0, 0.0, 15.0, 15.0]], dtype=np.float32),\n",
        "        }\n",
        "\n",
        "        _num_anchors = {\"stride32\": 2, \"stride16\": 2, \"stride8\": 2}\n",
        "        # ---------------------------\n",
        "        proposals_list = []\n",
        "        scores_list = []\n",
        "        landmarks_list = []\n",
        "\n",
        "        im_tensor, im_info, im_scale = preprocess.preprocess_image(img, allow_upscaling)\n",
        "\n",
        "\n",
        "        net_out = [elt.numpy() for elt in net_out]\n",
        "        sym_idx = 0\n",
        "\n",
        "        for _, s in enumerate(_feat_stride_fpn):\n",
        "            # _key = f\"stride{s}\"\n",
        "            scores = net_out[sym_idx]\n",
        "            scores = scores[:, :, :, _num_anchors[f\"stride{s}\"]:]\n",
        "\n",
        "            bbox_deltas = net_out[sym_idx + 1]\n",
        "            height, width = bbox_deltas.shape[1], bbox_deltas.shape[2]\n",
        "\n",
        "            A = _num_anchors[f\"stride{s}\"]\n",
        "            K = height * width\n",
        "            anchors_fpn = _anchors_fpn[f\"stride{s}\"]\n",
        "            anchors = postprocess.anchors_plane(height, width, s, anchors_fpn)\n",
        "            anchors = anchors.reshape((K * A, 4))\n",
        "            scores = scores.reshape((-1, 1))\n",
        "\n",
        "            bbox_stds = [1.0, 1.0, 1.0, 1.0]\n",
        "            bbox_pred_len = bbox_deltas.shape[3] // A\n",
        "            bbox_deltas = bbox_deltas.reshape((-1, bbox_pred_len))\n",
        "            bbox_deltas[:, 0::4] = bbox_deltas[:, 0::4] * bbox_stds[0]\n",
        "            bbox_deltas[:, 1::4] = bbox_deltas[:, 1::4] * bbox_stds[1]\n",
        "            bbox_deltas[:, 2::4] = bbox_deltas[:, 2::4] * bbox_stds[2]\n",
        "            bbox_deltas[:, 3::4] = bbox_deltas[:, 3::4] * bbox_stds[3]\n",
        "            proposals = postprocess.bbox_pred(anchors, bbox_deltas)\n",
        "\n",
        "            proposals = postprocess.clip_boxes(proposals, im_info[:2])\n",
        "\n",
        "            if s == 4 and decay4 < 1.0:\n",
        "                scores *= decay4\n",
        "\n",
        "            scores_ravel = scores.ravel()\n",
        "            order = np.where(scores_ravel >= threshold)[0]\n",
        "            proposals = proposals[order, :]\n",
        "            scores = scores[order]\n",
        "\n",
        "            proposals[:, 0:4] /= im_scale\n",
        "            proposals_list.append(proposals)\n",
        "            scores_list.append(scores)\n",
        "\n",
        "            landmark_deltas = net_out[sym_idx + 2]\n",
        "            landmark_pred_len = landmark_deltas.shape[3] // A\n",
        "            landmark_deltas = landmark_deltas.reshape((-1, 5, landmark_pred_len // 5))\n",
        "            landmarks = postprocess.landmark_pred(anchors, landmark_deltas)\n",
        "            landmarks = landmarks[order, :]\n",
        "\n",
        "            landmarks[:, :, 0:2] /= im_scale\n",
        "            landmarks_list.append(landmarks)\n",
        "            sym_idx += 3\n",
        "\n",
        "        proposals = np.vstack(proposals_list)\n",
        "        if proposals.shape[0] == 0:\n",
        "            return resp\n",
        "\n",
        "        scores = np.vstack(scores_list)\n",
        "        scores_ravel = scores.ravel()\n",
        "        order = scores_ravel.argsort()[::-1]\n",
        "        proposals = proposals[order, :]\n",
        "        scores = scores[order]\n",
        "        landmarks = np.vstack(landmarks_list)\n",
        "        landmarks = landmarks[order].astype(np.float32, copy=False)\n",
        "        pre_det = np.hstack((proposals[:, 0:4], scores)).astype(np.float32, copy=False)\n",
        "        keep = postprocess.cpu_nms(pre_det, nms_threshold)\n",
        "        det = np.hstack((pre_det, proposals[:, 4:]))\n",
        "        det = det[keep, :]\n",
        "        landmarks = landmarks[keep]\n",
        "        for idx, face in enumerate(det):\n",
        "            label = \"face_\" + str(idx + 1)\n",
        "            resp[label] = {}\n",
        "            resp[label][\"score\"] = face[4]\n",
        "            resp[label][\"facial_area\"] = list(face[0:4].astype(int))\n",
        "            resp[label][\"landmarks\"] = {}\n",
        "            resp[label][\"landmarks\"][\"right_eye\"] = list(landmarks[idx][0])\n",
        "            resp[label][\"landmarks\"][\"left_eye\"] = list(landmarks[idx][1])\n",
        "            resp[label][\"landmarks\"][\"nose\"] = list(landmarks[idx][2])\n",
        "            resp[label][\"landmarks\"][\"mouth_right\"] = list(landmarks[idx][3])\n",
        "            resp[label][\"landmarks\"][\"mouth_left\"] = list(landmarks[idx][4])\n",
        "        print(resp)\n",
        "        end = time.time()\n",
        "        print(f\" total  : {(end - start) * 1000} ms\")\n",
        "\n",
        "    def detect_faces_on_frames(self, frames:np.array):\n",
        "\n",
        "        for index in range (10):\n",
        "            time.sleep(0.001)\n",
        "            n_frames = frames.shape[0]\n",
        "            frames_out = self.model(frames)\n",
        "            for i in range(n_frames):\n",
        "                net_i_out = [\n",
        "                    tf.expand_dims(i_out[i], axis=0)\n",
        "                    for i_out in frames_out\n",
        "                ]\n",
        "                thread = threading.Thread(\n",
        "                    target=self.__determize_faces, args=(frames[i][:][:][:],net_i_out)\n",
        "                ).start()\n",
        "        print(\"finished\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7H1c_LYVBxi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YtX1Mc5IBxmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KOiJ8WEWBxpI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}